<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[面经--京东机器学习算法工程师]]></title>
    <url>%2F2018%2F04%2F16%2F%E9%9D%A2%E7%BB%8F--%E4%BA%AC%E4%B8%9C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%2F</url>
    <content type="text"><![CDATA[面经--京东机器学习算法工程师 共三面，两轮技术面，一轮hr面。 一面 1.自我介绍. 2.python装饰器，迭代器，生成器。 3.tensorflow高级操作的包，我回答contrib，他问我这个包的含义是什么,我不会，面试官告诉我是开源贡献者提供的方法。 4.机器学习的基础模型都有哪些？ 我答svm和logistic，讲了原理和损失函数的具体含义。 5.Inception和ResNet网络结构的特点和差别。 6.Hive和Hadoop了解吗？ 回答不了解，尴尬...... 7.问我为什么没过六级，...... 这个问题太难了，我也不知道怎么回答，后来面试官说没事，随口一问，吓死我了。 8.他问我有什么想问他的，我问了他所在的部门的具体工作，面试官说主要是做模型建立，给各个深度学习相关的部门提供底层模型支持。 一面结束，二十分钟。（太短了，很不适应） 二面 1.问了我知不知道应聘的部门是做什么的，感不感兴趣。 2.迄今为止最印象深刻的项目。 3.详细介绍了问了我简历上的三个项目。 4.最后给我一些建议，应该完善技术栈，会用并且知道怎么用，需要linux的命令经验，hadoop和spark需要会一些。 二面结束，还是二十分钟，面试官很nice，人很好。 hr面 1.自我介绍。 2.为什么从通信工程转到信息安全。 3.项目中遇到的最大的困难。 4.你能给你们部门带来什么。 hr面结束，还是二十分钟，第一次电话面试比较紧张。]]></content>
      <tags>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习--模型的评价指标]]></title>
    <url>%2F2018%2F04%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0--%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[分为回归问题的评价指标和分类问题的评价指标。 回归问题的评价指标： 1.SSE（误差平方和） 2.MAE(Mean Absolute Error) 平均绝对误差 3.MSE(Mean Square Error)平均平方差/均方误差是回归任务最常用的性能度量 4.MAPE 5.R-square（决定系数） 6.Adjusted R-Square (校正决定系数） 分类问题的评价指标： 1.accuarcy 正确率 2.precision,recall,F1-score,F-score,ROC-AUC曲线 回归问题 1.SSE（误差平方和） 2.MAE(Mean Absolute Error) 平均绝对误差 $$ MAE=\frac{1}{n}\sum_{n}^{i=1}|Y_predict-Y_actual| $$ 3.MSE(Mean Square Error) $$ MSE=\frac{1}{n}\sum_{n}{i=1}(Y_predict-Y_actual){2} $$ 4.MAPE 全称是Mean Absolute Percentage Error。 $$ MAPE=\frac{100}{n}\sum_{n}^{i=1}|\frac{Y_actual-Y_predict}{Y_actual}| $$ 5.R-square（决定系数） 6.Adjusted R-Square (校正决定系数） 回归问题 1.accuarcy 正确率 $$ accuarcy=\frac{N_true}{N_sum} $$ $N_true$是分类正确的数量 $N_sum$是训练样本的总数量 2.precision,recall,F1-score,F-score,ROC-AUC曲线 ROC曲线如下图，横轴是负正率，纵轴是真正率。 如何绘制曲线呢？ 真实标签y=[1,1,0,0,1] 分类器预测=[0.5,0.6,0.55,0.4,0.7] 我们选择阈值0.1，那么5个样本被分进1类，选择0.3，结果是一样的，选择0.45，那么只有样本4分为0类，其余都是1类，得到类别，我们就能计算真正率和负正率，就可以画图了。 理解ROC曲线： ROC曲线图中的四个点和一条线。第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。 对于ROC曲线图中的y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。 参考相关博客和资料：]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>评价指标</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面经--58同城算法工程师、优信二手车数据挖掘工程师]]></title>
    <url>%2F2018%2F04%2F12%2F%E9%9D%A2%E7%BB%8F--58%E5%90%8C%E5%9F%8E%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E3%80%81%E4%BC%98%E4%BF%A1%E4%BA%8C%E6%89%8B%E8%BD%A6%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%B7%A5%E7%A8%8B%E5%B8%88%2F</url>
    <content type="text"><![CDATA[面经--58同城算法工程师、优信二手车数据挖掘工程师 由于都是一面就结束，所以放在一起整理，如果有后续，会分开重新排版。 58同城算法工程师 一面 1.自我介绍。 2.自己的海量数据处理项目。 3.kaggle项目中用到的神经网络的不同，和更优秀的模型的差别。 4.机器学习的基础模型中svm的原理，线性不可分时还可以用svm吗？可以，用高斯核函数，无限延伸到无穷维度。 5.bagging和boosting的区别？ 一面结束，四十分钟左右，其中聊项目比较多，没问算法。 优信二手车数据挖掘工程师 一面 1.自我介绍。 2.自己JD店铺预测项目如何做的，用了什么模型？SVM,XGBOOST,GBDT... 3.SVM有哪些超参数？惩罚系数C和核函数选择和核函数的$ \sigma $。 使用SVM时，有两个点要注意： 若使用核函数，一定要对Feature做Feature Scaling(Normalization) 若训练集m太小，但Feature数量n很大，则训练数据不足以拟合复杂的非线性模型，这种情况下只能用linear-kernel（就是fi=xi）不能用高斯核 来自于 Andrew NG. machine learning class at coursera 4.logistic回归的超参数？只有一个：学习率 5.针对于回归问题的评价指标？当时没答上来，应该是mse，mae，决定系数等等。 5.AUC和ROC怎么求？概念不是很清晰，没推导出来 6.bagging和boosting的区别？ 7.bagging和boosting分别对模型的偏差和方差有着怎样的影响？ 简单来说，就是bagging方差降低了，偏差高。boosting是对偏差进行拟合，降低了偏差，方差会随着训练增大。 图片和结论来自于Michael_Shentu的博客 时间复杂度和空间复杂度的含义？ 算法题： 1.求一个字符串的所有子集？ 简单递归思想 2.二分查找 一面结束，五十分钟，答的一般，有两个问题不会。 一面结束让我等一下，进行二面，过了一会说二面不在，到时候再约面试，我说ok。 晚上hr小姐姐给我打电话，本来以为要约二面，结果和我说不用了，等下个月通知，一脸懵逼。]]></content>
      <tags>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习--最大似然估计（MLE）和最大后验概率（MAP）]]></title>
    <url>%2F2018%2F04%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0--%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%EF%BC%88MLE%EF%BC%89%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%88MAP%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最大似然估计（MLE）和最大后验概率（MAP） 最大似然估计 MLE 给定一堆数据，加入我们知道他是从某一分部中随机取出来的。可是我们并不知道这个分布具体的参，即“模型已定，参数未知”。例如，我们知道这个分布是正态分布，但是不知道均值和方差；或者是二项分布，但是不知道均值。最大似然估计（MLE，Maximum Likelihood Estimation）就可以用来估计模型的参数。MLE的目标是找出一组参数，使得模型产生出观测数据的概率最大： $ \arg max _{ \mu}(p(X; \mu)) $ 其中$ p(X; \mu) $ 就是似然函数，表示在参数$ \mu $ 下出现观测数据的概率。我们假设每个观测数据是独立的，那么有 为了寻求方便，一般对目标取$ log $。所以最优化似然函数等于最优化对数似然函数： 举一个抛硬币的简单例子。 现在有一个正反面不是很匀称的硬币，如果正面朝上记为正，方面朝上记为反，抛10次的结果如下： 正正正反反反正反正反 很显然这个概率是0.5。现在我们用MLE的思想去求解它。我们知道每次抛硬币都是一次二项分布，设正面朝上的概率是$ \mu $，那么似然函数为： x=1表示正面朝上，x=0表示方面朝上。那么有： 求导： 令导数为0，很容易得到： 也就是0.5。 最大后验概率 MAP 以上求MLE 求得是找出一组能够使似然函数最大的参数，即$ \arg max { \mu}(p(X; \mu)) $ 。现在问题稍微复杂一点，假如这个参数$ \mu $ 有一个先验概率呢？比如说，在上面抛硬币的例子，假如我们的经验告诉我们，硬币一般都是均匀的，也就是$ \mu $=0.5的可能性最大，$ \mu $=0.2的可能性比较小，那么参数如何估计呢？这就是MAP要考虑的问题。MAP优化的是一个后验概率，即给定了观测值后是的$ \mu $最大的： 把上式根据贝叶斯公式展开： 我们可以看出第一项$ p(X; \mu) $ 就是似然函数，第二项$ p(\mu) $就是先验知识。取$ log $之后就是： 回到刚才的抛硬币的例子，假设参数$ \mu $有一个先验估计，他服从$ Beta $分布，即： 而每次抛硬币仍然服从二项分布： 那么目标函数的导数是： 求导的第一项已经在MLE中给出了，第二项为： 令导数为0，求解为： 其中，$ n{H} $表示正面朝上的次数。这里看以看出，MLE与MAP的不同之处在于，MAP的结果多了一些先验分布的参数。 补充知识： Beta分布 Beat分布是一种常见的先验分布，它形状由两个参数控制，定义域为[0,1] Beta分布的最大值是x等于$ \frac{\alpha -1}{\alpha +\beta -2} $的时候： 所以在抛硬币中，如果先验知识是说硬币是匀称的，那么就让$ \alpha = \beta $。但是很显然即使它们相等，它两的值也对最终结果很有影响。它两的值越大，表示偏离匀称的可能性越小： 文章转自 Leavingseason的博客]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习--模型融合方法]]></title>
    <url>%2F2018%2F04%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0--%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Ensemble Learning--模型融合 含义： 通过对多个单模型融合以提升整体性能。 1.Voting 2.Averaging 3.Ranking 4.Bagging 5.Boosting 6.Bagging和Boosting的区别 7.Stacking 8.Blending 9.Stacking和Blending的区别 模型的融合条件 [x] Base Model 之间的相关性要尽可能的小。这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。 [x] Base Model 之间的性能表现不能差距太大。这其实是一个 Trade-off，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。 1.Voting 投票即为，投票多者为最终的结果。 用于分类问题： 多个模型投票（当然可以设置权重）。最终投票数最多的类为最终被预测的类。 2.Averaging Averaging即所有预测器的结果平均。 [ ] 回归问题，直接取平均值作为最终的预测值。（也可以使用加权平均） [ ] 分类问题，直接将模型的预测概率做平均。（or 加权） 加权平均，其公式如下： 其中n表示模型的个数， $Weighti$表示该模型权重，$Pi$表示模型i的预测概率值。 3.Ranking Rank的思想其实和Averaging一致，但Rank是把排名做平均，对于例如一些评价指标有效，如：AUC指标。 具体公式如下： 加权平均，其公式如下： 其中n表示模型的个数， $Weighti$表示该模型权重，所有权重相同表示平均融合。$Ranki$表示样本在第i个模型中的升序排名。它可以较快的利用排名融合多个模型之间的差异，而不需要加权融合概率。 4.Bagging 1.同一个学习算法在来自同一分布的多个不同的训练数据集上训练得到的模型偏差可能较大，即模型的方差（variance）较大，为了解决这个问题，可以综合多个模型的输出结果，对于回归问题可以取平均值，对于分类问题可以采取多数投票的方法。这就是Bagging的核心思想。 2.Bagging(Bootstrap Aggregation)是常用的统计学习方法，其综合的基本学习器可以是各种弱学习器。 3.使用训练数据的不同随机子集来训练每个 Base Model，最后每个 Base Model 权重相同，分类问题进行投票，回归问题平均。经典算法：随机森林 注意： 有放回抽样 （可能抽到重复的样本） Bagging是将弱分类器组装成强分类器的方法。 具体步骤： A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的） B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等） C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同） 如图所示： 要想综合N个弱分类器（决策树）的结果，我们需要采样N个训练数据集，在实际应用中获取N个训练数据集往往不现实，BootStrap 采样提供了一种有效的解决方法。 采用这样的方式解决了获取N个服从同一分布的原始数据集不现实的问题，而且在可接受程度上，可以认为Bootstrap 采样方式不影响到模型的准确性（以方差来衡量），即可以等价于使用N个不同的原始数据集。 Bagging较单棵决策树来说，降低了方差，但由于将多棵决策树的结果进行了平均，这损失了模型的可解释性。 5.boosting Boosting是一种迭代的方法，每一次训练会更关心上一次被分错的样本。 核心问题： 1）在每一轮如何改变训练数据的权值或概率分布？ 通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。 2）通过什么方式来组合弱分类器？ 通过加法模型将弱分类器进行线性组合 经典算法： AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。 提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。 6.Bagging和Boosting的区别 Bagging和Boosting的区别： 1）样本选择上： Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。 Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。 2）样例权重： Bagging：使用均匀取样，每个样例的权重相等 Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。 3）预测函数： Bagging：所有预测函数的权重相等。 Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。 4）并行计算： Bagging：各个预测函数可以并行生成 Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。 这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。 下面是将决策树与这些算法框架进行结合所得到的新的算法： 1）Bagging + 决策树 = 随机森林 2）AdaBoost + 决策树 = 提升树 3）Gradient Boosting + 决策树 = GBDT 7.Stacking 图片来自于Leon的知乎专栏 个人认为很想深度学习模型中的xception网络，这里不同的模型相当于xception中不同尺寸的卷积，进行不同特征的提取。 我们在这里只考虑两层的stacking，多层同理。 假设第一层的用到3个模型model1，model2，model3，第二层一个模型model4。 步骤： 首先先将训练集分成用K fold 用model对K-1折进行训练，剩下1折进行预测，一共可以的到K个（训练数据总数n/k）个结果，我们将其按照顺序排好，第1个n/k对应K折中的1折。reshape成为一个n*1的预测结果（我们可以把他考虑成为特征向量） 针对于预测集，K折交叉验证的每一次，都要对预测集进行一次预测，一共可以得到K*（预测集总数m），我们将其取平均，得到一个1*m的预测label（也可以看做是预测集的特征向量） 对于第一层的每个模型model1、model2、model3，我们都进行步骤1、2、3的操作，我们最终得到了训练集的特征向量是3*n（3是第一层的模型1数），得到预测集的特征向量是3*m。 对于第二层，我们用第一层得到的3*n作为特征用model4进行训练，模型训练结束后对 3*m进行预测，得到最终的结果。 最顶层的模型一般是LR或者线性模型。 8.Blending Blending与Stacking大致相同，只是Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模型的特征，而是建立一个Holdout集，例如说10%的训练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合。说白了，就是把Stacking流程中的K-Fold CV 改成 HoldOut CV。 举例： 1.将数据划分成train,test，然后将train划分成不相交的两部分train_1,train_2 2.使用不同的模型对train_1训练，对train_2和test预测，生成两个1维向量，有多少模型就生成多少维向量 3.第二层使用前面模型对train_2生成的向量和label作为新的训练集，使用LR或者其他模型训练一个新的模型来预测test生成的向量 9.Stacking和Blending的区别 stacking由于加入了K-fold，更加复杂；blending不用K-fold，所以更加简单 stacking因为使用K-fold，所以训练集的数据分布和原数据集的不一样了，会引入估计偏差；而blending不会。 blending用的数据会过少，多层之后，有可能会过拟合；而stacking不会出现。 两种方法都可以用下图来表示： 参考相关博客和资料： 1: https://blog.csdn.net/shine19930820/article/details/75209021#11-voting 2: https://zhuanlan.zhihu.com/p/26890738 3: https://www.cnblogs.com/liuwu265/p/4690486.html 4: https://blog.csdn.net/sinat_29819401/article/details/71191219 5: https://blog.csdn.net/bryan__/article/details/51229032 6: https://blog.csdn.net/foolsnowman/article/details/51726007]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>模型融合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面经--瓜子二手车数据分析岗位]]></title>
    <url>%2F2018%2F04%2F01%2F%E9%9D%A2%E7%BB%8F--%E7%93%9C%E5%AD%90%E4%BA%8C%E6%89%8B%E8%BD%A6%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B2%97%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[面经--瓜子二手车数据分析算法工程师 面试约的上午九点半，去了直接面试，估计去的太早了。 一面 1.自我介绍. 2.介绍一下机器学习模型都有哪些，判别模型和生成模型区别，具体有哪些。 3.推导logistic的损失函数。 4.什么是过拟合，过拟合如何解决。 5.什么是gbdt，gbdt如何确定第一棵树，没答出来，她换了个问法，说给你两个特征x，x1是连续的，x2是非连续的，和对应的label，具体的构建gbdt的方法。 6.描述kmeans算法，难度加深，给定一个每个点和其他点的距离而不是坐标，该如何聚类。 7.最后一道算法题：不用内置函数求sqrt，给定x和 $\Sigma$, 其中$ \Sigma $是误差项。应该用二分查找，但是最后没写出来。 一面结束，三十分钟左右。 二面 1.自我介绍。 2.什么是极大似然估计，如何推导。 3.什么是最大后验概率，如何推导。 4.描述一下朴素贝叶斯。 5.算法题：旋转数组中找到x。leetcode33题 6.1求概率：得分问题，赢一局得一分，输一局，不扣分，赢和输概率为p和q，n局得到m分的概率是。 6.2前面的问题难度加大，输一局扣一分，求n局得到m分的概率是。 6.3难度再提升，扣为0分不降分，n局得到m分的概率是。当时没做出来（应该用动态规划） 7.1用一枚硬币（1/2的概率）求出一个1/3的概率。 7.2用一枚硬币（1/2的概率）求出一个1/3的概率。 7.3用不均匀一枚硬币（概率不为1/2）求出一个1/3的概率。 二面面试官问我有什么要问的，我问了一下他所在的部门是做什么的，都有什么方向，他说在做分类推荐。二面结束，共四十分钟左右，面试官很和善，但是他出的问题都不是我自己独立解答的，80%都需要他的一些指引，认识到了自己的不足，继续努力。]]></content>
      <tags>
        <tag>面经</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习--L1正则化项和L2正则化项理解]]></title>
    <url>%2F2018%2F04%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0--L1%E6%AD%A3%E5%88%99%E5%8C%96%E9%A1%B9%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%8C%96%E9%A1%B9%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[L1正则化项和L2正则化项的理解 概念： L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为$ ||W||_{1} $ L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为$ ||W||_{2} $ 先抛出个结论： L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择,L1范数. L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的正则项$ ||W||_{2} $最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，大比起1范数，更常用L2范数。 通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合；一定程度上，L1也可以防止过拟合 L1正则化 为什么L1正则化可以产生稀疏模型（L1是怎么让系数等于零的），以及为什么L2正则化可以防止过拟合？ 假设有如下带L1正则化的损失函数： 其中$ J0 $是原始的损失函数，加号后面的一项是$ L1 $正则化项，$ α $是正则化系数。注意到$ L1 $正则化是权值的绝对值之和，$J$是带有绝对值符号的函数，因此$J$是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数$J0$后添加$L1$正则化项时，相当于对$J0$做了一个约束。令$L=α∑w|w|$，则$J=J0+L$，此时我们的任务变成在$L$约束下求出$J0$取最小值的解。考虑二维的情况，即只有两个权值$w1$和$w2$，此时$L=|w1|+|w2|$对于梯度下降法，求解$J0$的过程可以画出等值线，同时$L1$正则化的函数$L$也可以在$w1w2$的二维平面上画出来。如下图 图中等值线是$J0$的等值线，黑色方形是$L$函数的图形。在图中，当$J0$等值线与$L$图形首次相交的地方就是最优解。上图中$J0$与$L$在$L$的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是$(w1,w2)=(0,w)$。可以直观想象，因为$L$函数有很多『突出的角』（二维情况下四个，多维情况下更多），$J0$与这些角接触的机率会远大于与$L$其它部位接触的机率，而在这些角上，会有很多权值等于$0$，这就是为什么$L1$正则化可以产生稀疏模型，进而可以用于特征选择。 而正则化前面的系数$α$，可以控制L图形的大小。$α$越小，$L$的图形越大（上图中的黑色方框）；$α$越大，$L$的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优点的值$(w1,w2)=(0,w)$中的$w$可以取到很小的值。 L2正则化 假设有如下带L2正则化的损失函数： 同样可以画出他们在二维平面上的图形，如下： 二维平面下L2正则化的函数图形是个圆，与方形相比，被磨去了棱角。因此J0与L相交时使得w1或w2等于零的机率小了许多，这就是为什么L2正则化不具有稀疏性的原因。 L2正则化和过拟合 拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。 那为什么L2正则化可以获得值很小的参数？ 以线性回归中的梯度下降法为例。假设要求的参数为$θ$，$hθ(x)$是我们的假设函数，那么线性回归的代价函数如下： 以线性回归中的梯度下降法为例。假设要求的参数为$θ，hθ(x)$是我们的假设函数，那么线性回归的代价函数如下： 那么在梯度下降法中，最终用于迭代计算参数θ的迭代式为： 其中$α$是learning rate. 上式是没有添加L2正则化项的迭代公式，如果在原始代价函数之后添加L2正则化，则迭代公式会变成下面的样子： 其中$λ$就是正则化参数。从上式可以看到，与未添加$L2$正则化的迭代公式相比，每一次迭代，$θj$都要先乘以一个小于1的因子，从而使得$θj$不断减小，因此总得来看，$θ$是不断减小的。 最开始也提到$L1$正则化一定程度上也可以防止过拟合。之前做了解释，当$L1$的正则化系数很小时，得到的最优解会很小，可以达到和$L2$正则化类似的效果。 正则化参数的选择 通常越大的$λ$可以让代价函数在参数为$0$时取到最小值。 分别取$λ=0.5$和$λ=2$，可以看到越大的$λ$越容易使$F(x)$在$x=0$时取到最小值。 $λ$越大，$θj$衰减得越快。另一个理解可以参考上图，$λ$越大，$L2$圆的半径越小，最后求得代价函数最值时各参数也会变得很小。 除了：L1和L2 regularization外，正则化常用的方法还有：数据集扩增、dropout 文章转自 阿拉丁吃米粉的博客]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面经--小米机器学习岗位]]></title>
    <url>%2F2018%2F03%2F31%2F%E9%9D%A2%E7%BB%8F--%E5%B0%8F%E7%B1%B3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B2%97%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[面经--小米机器学习算法工程师 感谢两位校友@Marcovaldo和@KillersDeath提供的面经。 面试约的下午三点，第一次面试，心里有点紧张。 一面 1.自我介绍. 2.介绍一下机器学习模型都有哪些。 3.深度学习网络的网络特性和不同，主要说了Alexnet和ResNet的主要特征。说的时候问到是不是用layers直接写，我说最早也用conv和pooling，relu写过浅层的网络。 4.梯度爆炸和梯度消失，梯度爆炸当时没理解清，没回答的很明白。这个解释很好：梯度爆炸 5.项目上对如何找特征，用了什么模型和技巧，损失函数用的什么。我说用的gbdt，xgboost和SVM，他问我gbdt和SVM哪个对特征不需要处理（gbdt）， xgboost和gbdt的区别，回答速度快。问为什么xgboost速度快，回答用了二阶导数，问xgboost可以自定义损失函数，那你比赛利用的损失函数是什么，你对你的损失函数求一下二阶导，这坑挖的，算了半天没算出来。 6.L1、L2正则化的区别，为什么会产生稀疏性，为什么会降低特征权重。 7.给1张5*5*3的图片，3*3*64的卷积，用到了多少参数。（应该是想问我卷积层参数共享） 8.最后一道算法题：排序数组中给定某个重复出现数字第一次出现的下标。 剑指offer原题，算了半天用了两次二分查找，才写出来，面试官估计不咋满意。 一面结束，四十分钟左右。 二面 面试官应该是C++大神，想问我指针和数据流方面的题，但我都不会。于是问了三道算法题和一些小问题： 1.一个n*m的矩阵，所有数无顺序，求第k大的数。想用快排，面试官说可以，但是不是最好的。最后没想出来，结束后和同学讨论应该是堆排序加上mapreduce。 2.反转链表。剑指offer原题。 3.中间问了一下函数的传参问题。 4.问了堆排序。 5.旋转数组找k。leetcode33题。 二面结束，三十分钟左右，写算法特别慢，估计挂了。]]></content>
      <tags>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法基础--回文串问题]]></title>
    <url>%2F2018%2F03%2F25%2F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80--%E5%9B%9E%E6%96%87%E4%B8%B2%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[什么是回文palindrome？ 回文指的是正读和反读都一样的字符串，如aba，abba等 字符子串和字符子序列的区别? 字符字串指的是字符串中连续的n个字符；如palindrome中，pa，alind，drome等都属于它的字串 而字符子序列指的是字符串中不一定连续但先后顺序一致的n个字符；如palindrome中，plind，lime属于它的子序列，而mod，rope则不是，因为它们与字符串的字符顺序不一致。 最长回文子序列 题目要求： 给定字符串，求它的最长回文子序列长度。回文子序列反转字符顺序后仍然与原序列相同。例如字符串abcdfcba中，最长回文子序列长度为7，abcdcba或abcfcba。 解题思路： 动态规划 头尾字符串分别为start和end 如果首尾字符串相同，即s[start] = s[end] 那么最长回文子序列就等于dp[start+1][end-1] + 2 如果首尾字符串不同，那么最长回文子序列就在dp[start+1][end]和dp[start][end-1]之间 状态初始条件： $$dp[start][end]=1 ， start=end$$ 状态转移方程： $$ dp[start][end]=dp[start+1][end-1] + 2 , if（str[start]==str[end]） $$ $$ dp[start][end]=max(dp[start+1][end], dp[start][end-1]) , if（str[start]！=str[end]） $$ 注意： 当start == end时，dp[start][end]=1,即它本身 end 应该大于等于start 最重要的是要考虑start和end循环的方向，由于dp[start][end]和dp[start+1][end]、dp[start][end-1]、dp[start+1][end-1]有关系，我们可以得出结论是start是从大到小进行循环，end是从小到大进行循环 def sol(s): n = len(s) if n == 0 or n==1: return n dp = [[0]*n for _ in range(n)] for start in range(n-1, -1, -1): dp[start][start] = 1 for end in range(start+1, n, 1): if s[start] == s[end]: dp[start][end] = dp[start+1][end-1] + 2 else: dp[start][end] = max(dp[start+1][end], dp[start][end-1]) return dp[0][n-1]print(sol('abcdfcba')) 回文子序列个数 题目要求： 给定字符串，求它的回文子序列个数。回文子序列反转字符顺序后仍然与原序列相同。例如字符串aba中，回文子序列为&quot;a&quot;, &quot;a&quot;, &quot;aa&quot;, &quot;b&quot;, &quot;aba&quot;，共5个。内容相同位置不同的子序列算不同的子序列。 解题思路： 动态规划 对于任意字符串，如果头尾字符不相等，则字符串的回文子序列个数就等于去掉头的字符串的回文子序列个数+去掉尾的字符串的回文子序列个数-去掉头尾的字符串的回文子序列个数； 如果头尾字符相等，那么除了上述的子序列个数之外，还要加上首尾相等时新增的子序列个数，1+去掉头尾的字符串的回文子序列个数，1指的是加上头尾组成的回文子序列，如aa，bb等。 状态初始条件： $$dp[start][end]=1 ， start=end$$ 状态转移方程： $$ dp[start][end]=dp[start+1][end] + dp[start][end-1]-dp[start+1][end-1]+dp[start+1][end-1] + 1 ==dp[start+1][end] + dp[start][end-1] + 1, if（str[start]==str[end]） $$ $$ dp[start][end]=dp[start+1][end] + dp[start][end-1]-dp[start+1][end-1] , if（str[start]！=str[end]） $$ 当start == end时，dp[start][end]=1,即它本身 end 应该大于等于start start依然是从大到小进行循环，end依然是从小到大进行循环 def sol(s): n = len(s) if n == 0 or n==1: return n dp = [[0]*n for _ in range(n)] for start in range(n-1, -1, -1): dp[start][start] = 1 for end in range(start+1, n, 1): if s[start] == s[end]: dp[start][end] = dp[start+1][end]+ dp[start][end-1] + 1 else: dp[start][end] = dp[start+1][end]+ dp[start][end-1] -dp[start+1][end-1] return dp[0][n-1]print(sol('xxy')) 最大回文子串的长度 leetcode 题目要求： 给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 长度最长为1000。 示例： 输入: &quot;babad&quot; 输出：&quot;bab&quot; 注意: &quot;aba&quot;也是有效答案 解题思路： 1.最快的方法：O(N) --- 经典的'马拉车算法' 详细请点击 #!/usr/bin/python# -*- coding: utf-8 -*-def sol(s): ss = '$#' for i in s: ss += i ss += '#' return ssdef Manacher(s): ss = sol(s) n = len(ss) p = [0]*n idd = 0 mx = 0 for i in range(n): if i &lt; mx: if 2*idd - i &gt;-1: p[i] = min(p[2*idd-i], mx-i) else: p[i] = mx-i else: p[i] = 1 while (i+p[i]&lt;n and i-p[i]&gt;=0) and ss[i+p[i]] == ss[i-p[i]]: p[i] += 1 if p[i] + i &gt; mx: mx = p[i]+i idd = i return max(p)-1 print(Manacher('zedyxabaxy')) 解题思路： 2.动态规划：O(N^2) start &gt;= end 状态初始条件： $$dp[start][end]=True ， start=end$$ 状态转移方程： $$ dp[start][end]= str[start] == str[end] end-strart=1 $$ $$ dp[start][end]= str[start]==str[end]* And * dp[start+1][end-1] end - start&gt;1 $$ #!/usr/bin/python# -*- coding: utf-8 -*-def sol(s): n = len(s) dp = [[False]*n for _ in range(n)] for i in range(n-1, -1, -1): dp[i][i] = True for j in range(i+1, n): if j-i == 1: if s[i]==s[j]: dp[i][j] = True else: if i+1 &lt;= j-1: dp[i][j] = (s[i] == s[j]) and (dp[i+1][j-1]) maxx = 0 #print(dp) for i in range(n): for j in range(i,n): if dp[i][j] and j-i+1 &gt;maxx: maxx = j-i+1 return maxx print(sol('xdadadxy')) 回文链表 原链接 解题思路： 仔细观察一个回文比如：1 2 3 4 5 5 4 3 2 1发现有什么规律？假象把这个链表对折，那么相应的每个对称的数字都对的上（这不废话~），其实这个对折过程就是先把链表的一半（前一半后一半都行）反转然后在匹配的过程，如果是回文当然能一一对应上啦]]></content>
      <tags>
        <tag>算法</tag>
        <tag>回文串</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法基础--数学问题]]></title>
    <url>%2F2018%2F03%2F25%2F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80--%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[记录一下算法中出现的数学问题。 1.计算最小公倍数 2.计算平方根 3.不用加减乘除运算符做加法 1.计算最小公倍数 题目要求： 给定一个数n，求n之前（包括n）所有正整数的值得最小公倍数。 解题思路： 判断小于n的数里有多少个质数，质数由z1，z2，z3...zm组成，求得对于每一个质数z的$ int(log_{z}^{n}) $， 例如： 5 小于等于5的质数由[2,3,5] 答案是：2{int(log_{2}{5})}* 3{int(log_{3}{5})}* 5{int(log_{3}{5})} = 60 import math#!/usr/bin/python# -*- coding: utf-8 -*-def isPrime(n): if n&lt;=2: return n prime = [2] for i in range(3, n+1): sign = True m = int(math.sqrt(i)) for j in prime: if i%j == 0: sign = False break if sign: prime.append(i) return prime def logK(k, n): res = k while res &lt; n: res *= k if res&gt; n: return int(res/k) else: return resdef tosum(l, n): summ = 1 for i in l: summ *= logK(i, n) return summ def solve(n): l = isPrime(n) return tosum(l, n) print(solve(10)) ##例子 2.求平方根 题目要求： 给定一个数n和误差参数 $ \varepsilon $，求数n的平方根，并且误差不能大于 $ \varepsilon $。 解题思路： 二分查找的经典题。 每次start和end的中点mid的平方+e和目标值n做比较。 #!/usr/bin/python# -*- coding: utf-8 -*-def binary(s, start, end, e): mid = (start+end)/2.0 if mid**2+e &gt;=s and s&gt;=mid**2-e: return mid else: if mid**2+e &gt;s: return binary(s, start, mid,e) else: return binary(s, mid, end, e) def tosqrt(n, e): if n&lt;=1: return n start = 0 end = n return binary(n, start, end, e) print(tosqrt(10, 0.5)) ##测试 3. 不用加减乘除运算符做加法(剑指offer) 设计一个算法，不使用加减乘数运算符号实现加法运算。 解题思路： 位运算： 第一步：计算两个数的对应位，但是不计算进步，主要有四种情况，0+0=0，0+1=1，1+0=1，1+1=0，可以看出这正好对应异或操作。 第二步：计算两个数的进位，只有在1+1的情况下才会有进位，这正好对应与运算，计算完之后，每位上的数正好是后一位的进位，这时把结果左移一位即可。 第三步：把第一步的结果和第二步的结果相加，查看是否有进位，如果有进位重复第一步和第二步，直到没有进位为止。 注意：python如果数值大于32位，自动转化为long，我们不能让数值转换为long，做了一些限制。更为详细请点击： 参考python的int整型转化long #!/usr/bin/python# -*- coding: utf-8 -*-class Solution: def Add(self, a, b): ## python int最大值 MAX = 0x7FFFFFFF ## python int最小值 MIN = 0x80000000 ## 防止转化成long的标志数 mask = 0xFFFFFFFF while b != 0: a, b = (a ^ b) &amp; mask, ((a &amp; b) &lt;&lt; 1) &amp; mask ##转换 return a if a &lt;= MAX else ~(a ^ mask) ##防止越界]]></content>
      <tags>
        <tag>算法</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法基础--各种排序算法的性质]]></title>
    <url>%2F2018%2F01%2F31%2F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80--%E5%90%84%E7%A7%8D%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%B4%A8%2F</url>
    <content type="text"><![CDATA[算法种类 最好情况 平均情况 最坏情况 空间复杂度 是否稳定 直接插入排序 O(n) O(n^2) O(n^2) O(1) 是 冒泡排序 O(n) O(n^2) O(n^2) O(1) 是 简单选择排序 O(n^2) O(n^2) O(n^2) O(1) 否 希尔排序 O(1) 否 快速排序 O(nlogn) O(nlogn) O(n^2) O(logn) 否 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 否 2-路归并 O(nlogn) O(nlogn) O(nlogn) O(n) 是 基数排序 O(d(n+r)) O(d(n+r)) O(d(n+r)) O(r) 是 快速排序 快速排序是每趟都确定一个元素的位置，并且在它的位置左边的都比它小，在它右边的都比它大。 void QuickSort(ElemType A[], int low, int high)&#123; if(low &lt; high)&#123; int pivotpos = Partition(A, low, high); QuickSort(A, low, pivotpos-1); QuickSort(A, pivotpos+1, high); &#125;&#125;int Partition(ElemType A[], int low, int high)&#123; ElemType pivot = A[low]; while(low &lt; high)&#123; while(low&lt;high &amp;&amp; A[high] &gt; pivot) high--; A[low] = A[high]; while(low&lt;high &amp;&amp; A[low] &lt; pivot) low++; &#125; A[low] = pivot; return low;&#125; 堆排序 堆排序是先要构建成大顶堆，而后依次将root跟二叉树最后一个元素互换，每次互换后都要将二叉树再调整回大顶堆。 下面是建立大顶堆的过程： void BuildMaxHeap(ElemType A[], int len)&#123; for(i=len/2;i&gt;0;i--) AdjustDown(A, i, len);&#125;void AdjustDown(ElemType A[], int k, int len)&#123; A[0] = A[k]; for(i=2*k;i&lt;=len;i*=2)&#123; if(i&lt;len&amp;&amp;A[i]&lt;A[i+1]) i++; if(A[i] &gt; A[0]) A[k] = A[i]; k = i; else break; &#125;&#125; 下面是堆排序算法： void HeapSort(ElemType A[], int len)&#123; BuildMaxHeap(A, len); for(i=len;i&gt;1;i--)&#123; Swap(A[i], A[1]); AdjustDown(A, i-1); &#125;&#125; 堆支持删除和插入操作。删除堆顶元素时先将堆顶元素与最后一个元素互换，之后进行向下调整。插入元素时，直接将元素插入到最后的位置，之后进行向上调整。下面是执行向上调整的函数： void AdjustUp(ElemType A[], int k)&#123;//参数k为向上调整的结点，也就是堆中的元素个数 A[0] = A[k]; int i = k / 2; while(i&gt;0&amp;&amp;A[i]&lt;A[0])&#123; A[k] = A[i]; k = i; i = k/2; &#125; A[k] = A[0];&#125; 简单选择排序 假设排序表为L[1...n]，第i趟排序即从L[1...n]中选择关键字最小的元素与L[i]进行交换，每一趟排序都可以确定一个元素的最终位置，经过n-1趟排序就可以使整个排序表有序。 void SelectSort(ElemType A[], int n)&#123; for(i=0;i&lt;n-1;i++)&#123; min = i; for(j=i+1;j&lt;n;j++)&#123; if(A[j] &lt; A[min]) min = j; &#125; if(min!=i) Swap(A[i], A[min]); &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习--keras中image.ImageDataGenerator.flow_from_directory()方法]]></title>
    <url>%2F2017%2F07%2F31%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0--keras%E4%B8%ADimage.ImageDataGenerator.flow_from_directory()%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[作用：keras中image.ImageDataGenerator.flow_from_directory()方法可以实现从文件夹中提取图片和进行简单归一化处理。 keras中有很多封装好的API可以帮助我们实现对图片数据的读取和处理。 比如 ： keras.preprocessing.image.ImageDataGenerator.flow_from_directory( ) 这个函数 这个函数的参数包括： flow_from_directory(self, directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='jpeg', follow_links=False) flow_from_directory(directory): 以文件夹路径为参数,生成经过数据提升/归一化后的数据,在一个无限循环中无限产生batch数据 directory: 目标文件夹路径,对于每一个类,该文件夹都要包含一个子文件夹.子文件夹中任何JPG、PNG、BNP、PPM的图片都会被生成器使用.详情请查看此脚本 target_size: 整数tuple,默认为(256, 256). 图像将被resize成该尺寸 color_mode: 颜色模式,为”grayscale”,”rgb”之一,默认为”rgb”.代表这些图片是否会被转换为单通道或三通道的图片. classes: 可选参数,为子文件夹的列表,如[‘dogs’,’cats’]默认为None. 若未提供,则该类别列表将从directory下的子文件夹名称/结构自动推断。每一个子文件夹都会被认为是一个新的类。(类别的顺序将按照字母表顺序映射到标签值)。通过属性class_indices可获得文件夹名与类的序号的对应字典。 class_mode: “categorical”, “binary”, “sparse”或None之一. 默认为”categorical. 该参数决定了返回的标签数组的形式, “categorical”会返回2D的one-hot编码标签,”binary”返回1D的二值标签.”sparse”返回1D的整数标签,如果为None则不返回任何标签, 生成器将仅仅生成batch数据, 这种情况在使用model.predict_generator()和model.evaluate_generator()等函数时会用到. batch_size: batch数据的大小,默认32 shuffle: 是否打乱数据,默认为True seed: 可选参数,打乱数据和进行变换时的随机数种子 save_to_dir: None或字符串，该参数能让你将提升后的图片保存起来，用以可视化 save_prefix：字符串，保存提升后图片时使用的前缀, 仅当设置了save_to_dir时生效 save_format：”png”或”jpeg”之一，指定保存图片的数据格式,默认”jpeg” flollow_links: 是否访问子文件夹中的软链接 使用flow_from_directory最值得注意的是directory这个参数： directory: path to the target directory. It should contain one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator. 这是官方文档的定义，它的目录格式一定要注意是包含一个子目录下的所有图片这种格式，driectoty路径只要写到标签路径上面的那个路径即可。 target_size：可是实现对图片的尺寸转换，是预处理中比较常用的方法 save_to_dir: 可以设置保存处理后图片的路径。 save_prefix: 可以对处理后图片设置前缀。]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络安全--日志分析：报警关联]]></title>
    <url>%2F2017%2F07%2F03%2F%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8--%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%EF%BC%9A%E6%8A%A5%E8%AD%A6%E5%85%B3%E8%81%94%2F</url>
    <content type="text"><![CDATA[网络安全--日志分析：报警关联 警关联分析技术对以入侵报警信息为主的安全报警进行组合、解释和分析。目标是对报警信息的求精，以对攻击规划进行识别和场景重构。 报警关联是一个过程，分析了由一个或多个入侵检测系统产生的警报，提供了更加简洁和高层次的入侵或企图侵入的判断。尽管关联过程往往是由一个步骤提出，分析实际通过若干组成部分协作完成，各自有具体的目标。通过对来自于不同类型IDS和其他安全设备所产生的报警进行聚合与关联，可以有效地解决入侵检测分类技术由于硬件速度、算法假设和复杂网络环境的影响所很难克服的误报和漏报偏高等问题，为自动、深入的报警处理打下基础，可以实现如下目标： 1.消除或减少重复报警。网络上不同的IDS 针对同一个安全事件都可能报警，即使是同一个 IDS也可能对某个安全事件发出多个报警。这些重复报警少则几个，多则上万。通过对报警的聚合，可以大大减少重复报警率。 2.降低误报率。通过将一个攻击过程相关报警信息关联在一起，可以消除某些孤立和随机事件产生的误报警。另外，报警信息同被保护网络系统本身的信息相互比对就可以很好地滤除无关报警，降低误报率。 降低漏报率。目前，没有一种单一的 IDS 能够检测到所有网络攻击。不同 IDS 之间的报警通过关联，可以相互补充，防止漏报的发生。 3.发现高层攻击策略。将入侵过程的一系列攻击活动关联在一起，重建攻击过程，这样就可以对入侵的整体情况进行描述，有利于对入侵的理解，发现攻击策略，克服了以往IDS那种检测结果过于细化和底层缺点，为人侵的意图识别、入侵行动预测和入侵响应打下了基础。 4.扩大了IDS的检测范围。在较大型的交换式网络统中，单个的IDS(不论是基于网络的还是基于主机的)其检测范围和处理能力是有限的，要想掌握整个网络的安全情况，就要将多个 IDS和安全设备布置在网络上的不同位置，然后将来自于这些安全设备的报警进行融合处理，从而实现全网范围内的安全防御。 目前已经提出的入侵报警关联分析方法，主要可以分为报警聚类方法、基于攻击规划库的报警关联方法和基于攻击行为建模的报警关联方法三大类。 报警聚类方法 1.报警聚类方法属于最初步的关联分析，通过报警属性值之间的相似性对报警进行聚类，使得同个聚类的报警集合具有某些相同的特性，然后选择一个抽象的“元报警”事件作为该聚类的代表元。此类方法不能够完全揭示出相关报警之间的因果联系，无法对报警反映的攻击场景给出清晰的解释，也无法预测攻击者的目标和进一步的攻击规划。 基于攻击规划库的报警关联方法 2.基于攻击规划库的报警关联方法在拥有一个完整的已知攻击规划库的基础上，通过一系列关联分析技术来识别报警流中包含的与攻击规划库中的攻击规划相一致的攻击场景实例。基于攻击规划库的报警关联方法是基于封闭世界假说，需要一个完整的攻击规划库作为支撑基础，但在实际环境中，由于攻击动作的多样性和攻击过程的随意性以及新的攻击工具和技术的不断出现，使得构建一个完善的攻击规划库极为困难。此外，由于存在并行的攻击场景，上述的各种方法对每个攻击场景维护中间的状态，从而导致其存储代价和计算代价都非常庞大。上述两个缺陷使得基于攻击场景库的报警关联方法难以在实际中得到广泛的有效应用。 基于攻击行为建模的报警关联方法 3.基于攻击行为建模的报警关联方法通过对攻击动作的前提条件和造成的后果进行描述，构建攻击模型，如果一个攻击动作的后果使得另一攻击动作的前提条件得到满足，则认为这两个攻击动作之间存在因果关系。基于攻击行为建模的报警关联算法就利用这些因果关系将观察到的报警进行连接，构造出整个攻击场景。基于攻击行为建模的报警关联方法不需要对所有可能的攻击规划进行描述和存储，只需要对单个攻击行为动作进行建模，准确刻画其所需的前提条件和造成的后果，因此攻击知识模型的构建和维护较为可行，另外构建的攻击知识模型存在较强的灵活性，能够对一些未知的攻击场景进行识别。 现有的入侵报警关联分析技术还存在着如下一些问题： [ ] 首先，能够完成攻击规划识别的关联分析技术都需要有一个领域知识库的支撑，虽然目前在攻击关联知识模型方面有一些研究工作，但目前尚未有完整的、实际可行的网络攻防知识库的构建方案。攻防知识库构建的困难阻碍了关联分析技术的实际应用； [ ] 其次，现有的入侵报警关联分析技术大多仅仅根据固定的模式对报警进行关联，而未对受监控系统以及攻击者的状态进行有效跟踪； [ ] 第三，缺乏对攻击者意图的识别，无法为正确及时的响应提供支持；最后，由于在实际网络环境中很难获得攻击场景测试数据并准确地从背景网络流量中对其进行正确标识，因此对入侵报警关联分析技术的测试和验证普遍缺乏足够的攻击场景数据，这也使得目前大多数的入侵报警关联分析技术研究仅仅针对几个人工构造的攻击场景数据集，并不能够体现实用性。]]></content>
      <tags>
        <tag>网络安全</tag>
        <tag>日志分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习--机器学习与数学基础笔记整理]]></title>
    <url>%2F2017%2F03%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0--%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[机器学习 什么是机器学习？ 对于某给定的任务T，在合理的性能度量方案P的前提下，某计算机程序可以自主学习任务T的经验E；随着提供合适、优质、大量的静安E，改程序对于任务T的性能逐步提交。 这个任务最重要的是机器学习的对象： 任务Task，T，一个或者多个 经验Experience，E 性能Performance，P 总结来说：就是随着任务的不断执行，经验的累计会带来计算性能的提升。 机器学习的一般流程 数据收集 数据清洗 特征工程 数据建模 注意： 数据直接影响学习的结果。 对特征的选取不同，所得到的结果也会不同。 对于模型的选择，也有不同的方案，同样的数据用不同的模型所得到的结果不同。 机器学习的数学基础 高等数学 导数 简单来说，导数是曲线的斜率，是曲线变化快慢的反应 二阶导数是斜率变化快慢的反应，表征曲线凹凸性 根据$\lim_{x\rightarrow \infty }(1+\frac{1}{x})^{x}=e$，可以得到函数$ f(x)=lnx $的导数，进一步根据换底公式/反函数求导等，得到其他初等函数的导数。 常用函数的导数 $ {C}'=0$ ${({x}n)}'=nx{n-1} $ $ {sinx}'=cosx$ ${cosx}'=-sinx $ $ {(a{x})}'=a{x}lna$ ${(ex)}'=ex $ $ {(log_{a}x)}'=\frac{1}{x}{log_{a}e}$ ${(lnx)}'=\frac{1}{x} $ $ {(u+v)}'=u'+v'$ ${(uv)}'=u'v+uv' $ 例子：已知$ f(x)=x^x,x&gt;0 $，求解其最小值 $ t=x^x $ $ lnt=xlnx $ 两边对x求导:$ \frac{1}{t}t'=lnx+1 $ 令$ t'=0$:$lnx+1=0 $ $ x=(e)^{-1} $ $ t=e^{-\frac{1}{e}} $ 例子：推导$ N\rightarrow \infty \Rightarrow lnN!\rightarrow N(lnN-1) $ $ lnN!=\sum_{i=1}^{N}lni\approx \int_{1}^{N}lnxdx $ $ =xlnx|{1}{N}-\int_{1}{N}xdxlnx $ $ =NlnN-\int{1}^{N}x\cdot \frac{1}{x}dx $ $ =NlnN-x|_{1}^{N} $ $ =NlnN-N+1 $ $ \rightarrow NlnN-N $ Tayor公式 $ f(x)=f(x_{0})+f'(x_{0})(x-x_{0})+\frac{f''(x_{0})}{2!}(x-x_{0})+...+\frac{f{(n)}(x_{0})}{n!}(x-x_{0}){n}+R_{n}(x) $ 令x=0，得到： $ f(x)=f(0)+f'(0)x+\frac{f''(0)}{2!}x{2}+...+\frac{f{n}(0)}{n!}x{n}+o(x{n}) $ Taylor公式的应用： 数值计算：初等函数值计算（在原点上展开） $$ sinx=x-\frac{x{3}}{3!}+\frac{x{5}}{5!}-\frac{x{7}}{7!}+\frac{x{9}}{9!}+...+(-1){m-1}\frac{x{2m-1}}{(2m-1)!}+R_{2m} $$ $$ e{x}=1+x+\frac{x{2}}{2!}+\frac{x{3}}{3!}+\frac{x{4}}{4!}+...+\frac{x^{n}}{n!}+R_{n} $$ 在实践中，往往需要做一定程度上的变换。 例子：计算$ e^{x} $ $ x=k\cdot ln2+r $ , $ \left | r \right |\leqslant 0.5\cdot ln2 $ $$ e{x}=e{k\cdot ln2+r}=e^{k\cdot ln2}\cdot e^{r} =2^{k}\cdot e^{r} $$ 方向导数 如果函数$ z=f(x,y) $在点$ P(x,y) $是可微分的，那么，函数在该点沿任意方向$ L $的方向导数都存在，且有： $ \frac{\partial f}{\partial l}=\frac{\partial f}{\partial x}cos\varphi +\frac{\partial f}{\partial y}sin\varphi $ 其中，$ \varphi $为x轴到方向L的转角。 梯度 设函数$ z=f(x,y) $在平面区域D内具有一阶连续偏导数，则对于每一个点$P(x,y)\in D$,向量 $$ (\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}) $$ 为函数$ z=f(x,y) $在点$P(x,y)$的梯度，记作$ gradf(x) $ 梯度的方向是函数在该点变化最快的方向 ###概率论 概率表达式：$ P(x)\in [0,1] $ 若x为离散，则$ P(x=x_{0}) $表示$ x_{0} $发生的概率 若x为连续变量，则$ P(x=x_{0}) $表示$x_{0}$发生的概率密度 累计分布函数：$ \Phi (x)=P(x\leq x_{0}) $ $ \Phi (x) $一定为单调递增函数。 $ min(\Phi (x))=0 $,$ max(\Phi (x))=1 $ 思考：将值域为[0,1]的单增函数$ y=F(x) $看成X事件的累积概率函数，若$y=F(x)$可导，则$f(x)=F'(x)$为概率密度函数 概率公式 条件概率：$ P(A|B)=\frac{P(AB)}{P(B)} $ 全概率公式：$ P(A)=\sum_{i}P(A|B_{i})P(B_{i}) $ 贝叶斯(Bayes)公式：$ P(B_{i}|A)=\frac{P(A|B_{i})P(B_{i})}{\sum_{j}P(A|B_{i})P(B_{j})} $ 贝叶斯公式： 分布]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
