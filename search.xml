<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[瓜子二手车--数据分析岗位面经]]></title>
    <url>%2F2018%2F04%2F05%2F%E7%93%9C%E5%AD%90%E4%BA%8C%E6%89%8B%E8%BD%A6--%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%E5%B2%97%E4%BD%8D%E9%9D%A2%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[面试约的上午九点半，去了直接面试，估计去的太早了。 一面 1.自我介绍. 2.介绍一下机器学习模型都有哪些，判别模型和生成模型区别，具体有哪些。 3.推导logistic的损失函数。 4.什么是过拟合，过拟合如何解决。 5.什么是gbdt，gbdt如何确定第一棵树，没答出来，她换了个问法，说给你两个特征x，x1是连续的，x2是非连续的，和对应的label，具体的构建gbdt的方法。 6.描述kmeans算法，难度加深，给定一个每个点和其他点的距离而不是坐标，该如何聚类。 7.最后一道算法题：不用内置函数求sqrt，给定x和 $\Sigma$, 其中$ \Sigma $是误差项。应该用二分查找，但是最后没写出来。 一面结束，三十分钟左右。 二面 1.自我介绍。 2.什么是极大似然估计，如何推导。 3.什么是最大后验概率，如何推导。 4.描述一下朴素贝叶斯。 5.算法题：旋转数组中找到x。leetcode33题 6.1求概率：得分问题，赢一局得一分，输一局，不扣分，赢和输概率为p和q，n局得到m分的概率是。 6.2前面的问题难度加大，输一局扣一分，求n局得到m分的概率是。 6.3难度再提升，扣为0分不降分，n局得到m分的概率是。当时没做出来（应该用动态规划） 7.1用一枚硬币（1/2的概率）求出一个1/3的概率。 7.2用一枚硬币（1/2的概率）求出一个1/3的概率。 7.3用不均匀一枚硬币（概率不为1/2）求出一个1/3的概率。 二面面试官问我有什么要问的，我问了一下他所在的部门是做什么的，都有什么方向，他说在做分类推荐。二面结束，共四十分钟左右，面试官很和善，但是他出的问题都不是我自己独立解答的，80%都需要他的一些指引，认识到了自己的不足，继续努力。]]></content>
      <tags>
        <tag>面经</tag>
        <tag>瓜子</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小米--机器学习岗位面经]]></title>
    <url>%2F2018%2F03%2F31%2F%E5%B0%8F%E7%B1%B3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B2%97%E4%BD%8D%E9%9D%A2%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[感谢两位校友@Marcovaldo和@KillersDeath提供的面经。 面试约的下午三点，第一次面试，心里有点紧张。 一面 1.自我介绍. 2.介绍一下机器学习模型都有哪些。 3.深度学习网络的网络特性和不同，主要说了Alexnet和ResNet的主要特征。说的时候问到是不是用layers直接写，我说最早也用conv和pooling，relu写过浅层的网络。 4.梯度爆炸和梯度消失，梯度爆炸当时没理解清，没回答的很明白。这个解释很好：梯度爆炸 5.项目上对如何找特征，用了什么模型和技巧，损失函数用的什么。我说用的gbdt，xgboost和SVM，他问我gbdt和SVM哪个对特征不需要处理（gbdt）， xgboost和gbdt的区别，回答速度快。问为什么xgboost速度快，回答用了二阶导数，问xgboost可以自定义损失函数，那你比赛利用的损失函数是什么，你对你的损失函数求一下二阶导，这坑挖的，算了半天没算出来。 6.L1、L2正则化的区别，为什么会产生稀疏性，为什么会降低特征权重。 7.给1张5*5*3的图片，3*3*64的卷积，用到了多少参数。（应该是想问我卷积层参数共享） 8.最后一道算法题：排序数组中给定某个重复出现数字第一次出现的下标。 剑指offer原题，算了半天用了两次二分查找，才写出来，面试官估计不咋满意。 一面结束，四十分钟左右。 二面 面试官应该是C++大神，想问我指针和数据流方面的题，但我都不会。于是问了三道算法题和一些小问题： 1.一个n*m的矩阵，所有数无顺序，求第k大的数。想用快排，面试官说可以，但是不是最好的。最后没想出来，结束后和同学讨论应该是堆排序加上mapreduce。 2.反转链表。剑指offer原题。 3.中间问了一下函数的传参问题。 4.问了堆排序。 5.旋转数组找k。leetcode33题。 二面结束，三十分钟左右，写算法特别慢，估计挂了。]]></content>
      <tags>
        <tag>小米</tag>
        <tag>机器学习</tag>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法基础1:各种排序算法的性质]]></title>
    <url>%2F2018%2F01%2F31%2F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%801%EF%BC%9A%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[算法种类 最好情况 平均情况 最坏情况 空间复杂度 是否稳定 直接插入排序 O(n) O(n^2) O(n^2) O(1) 是 冒泡排序 O(n) O(n^2) O(n^2) O(1) 是 简单选择排序 O(n^2) O(n^2) O(n^2) O(1) 否 希尔排序 O(1) 否 快速排序 O(nlogn) O(nlogn) O(n^2) O(logn) 否 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 否 2-路归并 O(nlogn) O(nlogn) O(nlogn) O(n) 是 基数排序 O(d(n+r)) O(d(n+r)) O(d(n+r)) O(r) 是 快速排序 快速排序是每趟都确定一个元素的位置，并且在它的位置左边的都比它小，在它右边的都比它大。 void QuickSort(ElemType A[], int low, int high)&#123; if(low &lt; high)&#123; int pivotpos = Partition(A, low, high); QuickSort(A, low, pivotpos-1); QuickSort(A, pivotpos+1, high); &#125;&#125;int Partition(ElemType A[], int low, int high)&#123; ElemType pivot = A[low]; while(low &lt; high)&#123; while(low&lt;high &amp;&amp; A[high] &gt; pivot) high--; A[low] = A[high]; while(low&lt;high &amp;&amp; A[low] &lt; pivot) low++; &#125; A[low] = pivot; return low;&#125; 堆排序 堆排序是先要构建成大顶堆，而后依次将root跟二叉树最后一个元素互换，每次互换后都要将二叉树再调整回大顶堆。 下面是建立大顶堆的过程： void BuildMaxHeap(ElemType A[], int len)&#123; for(i=len/2;i&gt;0;i--) AdjustDown(A, i, len);&#125;void AdjustDown(ElemType A[], int k, int len)&#123; A[0] = A[k]; for(i=2*k;i&lt;=len;i*=2)&#123; if(i&lt;len&amp;&amp;A[i]&lt;A[i+1]) i++; if(A[i] &gt; A[0]) A[k] = A[i]; k = i; else break; &#125;&#125; 下面是堆排序算法： void HeapSort(ElemType A[], int len)&#123; BuildMaxHeap(A, len); for(i=len;i&gt;1;i--)&#123; Swap(A[i], A[1]); AdjustDown(A, i-1); &#125;&#125; 堆支持删除和插入操作。删除堆顶元素时先将堆顶元素与最后一个元素互换，之后进行向下调整。插入元素时，直接将元素插入到最后的位置，之后进行向上调整。下面是执行向上调整的函数： void AdjustUp(ElemType A[], int k)&#123;//参数k为向上调整的结点，也就是堆中的元素个数 A[0] = A[k]; int i = k / 2; while(i&gt;0&amp;&amp;A[i]&lt;A[0])&#123; A[k] = A[i]; k = i; i = k/2; &#125; A[k] = A[0];&#125; 简单选择排序 假设排序表为L[1...n]，第i趟排序即从L[1...n]中选择关键字最小的元素与L[i]进行交换，每一趟排序都可以确定一个元素的最终位置，经过n-1趟排序就可以使整个排序表有序。 void SelectSort(ElemType A[], int n)&#123; for(i=0;i&lt;n-1;i++)&#123; min = i; for(j=i+1;j&lt;n;j++)&#123; if(A[j] &lt; A[min]) min = j; &#125; if(min!=i) Swap(A[i], A[min]); &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习：keras中image.ImageDataGenerator.flow_from_directory()方法]]></title>
    <url>%2F2017%2F07%2F31%2Fkeras%E4%B8%ADimage.ImageDataGenerator.flow_from_directory()%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[作用：keras中image.ImageDataGenerator.flow_from_directory()方法可以实现从文件夹中提取图片和进行简单归一化处理。 keras中有很多封装好的API可以帮助我们实现对图片数据的读取和处理。 比如 ： keras.preprocessing.image.ImageDataGenerator.flow_from_directory( ) 这个函数 这个函数的参数包括： flow_from_directory(self, directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='jpeg', follow_links=False) flow_from_directory(directory): 以文件夹路径为参数,生成经过数据提升/归一化后的数据,在一个无限循环中无限产生batch数据 directory: 目标文件夹路径,对于每一个类,该文件夹都要包含一个子文件夹.子文件夹中任何JPG、PNG、BNP、PPM的图片都会被生成器使用.详情请查看此脚本 target_size: 整数tuple,默认为(256, 256). 图像将被resize成该尺寸 color_mode: 颜色模式,为”grayscale”,”rgb”之一,默认为”rgb”.代表这些图片是否会被转换为单通道或三通道的图片. classes: 可选参数,为子文件夹的列表,如[‘dogs’,’cats’]默认为None. 若未提供,则该类别列表将从directory下的子文件夹名称/结构自动推断。每一个子文件夹都会被认为是一个新的类。(类别的顺序将按照字母表顺序映射到标签值)。通过属性class_indices可获得文件夹名与类的序号的对应字典。 class_mode: “categorical”, “binary”, “sparse”或None之一. 默认为”categorical. 该参数决定了返回的标签数组的形式, “categorical”会返回2D的one-hot编码标签,”binary”返回1D的二值标签.”sparse”返回1D的整数标签,如果为None则不返回任何标签, 生成器将仅仅生成batch数据, 这种情况在使用model.predict_generator()和model.evaluate_generator()等函数时会用到. batch_size: batch数据的大小,默认32 shuffle: 是否打乱数据,默认为True seed: 可选参数,打乱数据和进行变换时的随机数种子 save_to_dir: None或字符串，该参数能让你将提升后的图片保存起来，用以可视化 save_prefix：字符串，保存提升后图片时使用的前缀, 仅当设置了save_to_dir时生效 save_format：”png”或”jpeg”之一，指定保存图片的数据格式,默认”jpeg” flollow_links: 是否访问子文件夹中的软链接 使用flow_from_directory最值得注意的是directory这个参数： directory: path to the target directory. It should contain one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator. 这是官方文档的定义，它的目录格式一定要注意是包含一个子目录下的所有图片这种格式，driectoty路径只要写到标签路径上面的那个路径即可。 target_size：可是实现对图片的尺寸转换，是预处理中比较常用的方法 save_to_dir: 可以设置保存处理后图片的路径。 save_prefix: 可以对处理后图片设置前缀。]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[邹博机器学习课程第一讲：机器学习与数学基础笔记整理]]></title>
    <url>%2F2017%2F07%2F03%2F%E9%82%B9%E5%8D%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%B8%80%E8%AE%B2%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[机器学习 什么是机器学习？ 对于某给定的任务T，在合理的性能度量方案P的前提下，某计算机程序可以自主学习任务T的经验E；随着提供合适、优质、大量的静安E，改程序对于任务T的性能逐步提交。 这个任务最重要的是机器学习的对象： 任务Task，T，一个或者多个 经验Experience，E 性能Performance，P 总结来说：就是随着任务的不断执行，经验的累计会带来计算性能的提升。 机器学习的一般流程 数据收集 数据清洗 特征工程 数据建模 注意： 数据直接影响学习的结果。 对特征的选取不同，所得到的结果也会不同。 对于模型的选择，也有不同的方案，同样的数据用不同的模型所得到的结果不同。 机器学习的数学基础 高等数学 导数 简单来说，导数是曲线的斜率，是曲线变化快慢的反应 二阶导数是斜率变化快慢的反应，表征曲线凹凸性 根据$\lim_{x\rightarrow \infty }(1+\frac{1}{x})^{x}=e$，可以得到函数$ f(x)=lnx $的导数，进一步根据换底公式/反函数求导等，得到其他初等函数的导数。 常用函数的导数 $ {C}'=0$ ${({x}n)}'=nx{n-1} $ $ {sinx}'=cosx$ ${cosx}'=-sinx $ $ {(a{x})}'=a{x}lna$ ${(ex)}'=ex $ $ {(log_{a}x)}'=\frac{1}{x}{log_{a}e}$ ${(lnx)}'=\frac{1}{x} $ $ {(u+v)}'=u'+v'$ ${(uv)}'=u'v+uv' $ 例子：已知$ f(x)=x^x,x&gt;0 $，求解其最小值 $ t=x^x $ $ lnt=xlnx $ 两边对x求导:$ \frac{1}{t}t'=lnx+1 $ 令$ t'=0$:$lnx+1=0 $ $ x=(e)^{-1} $ $ t=e^{-\frac{1}{e}} $ 例子：推导$ N\rightarrow \infty \Rightarrow lnN!\rightarrow N(lnN-1) $ $ lnN!=\sum_{i=1}^{N}lni\approx \int_{1}^{N}lnxdx $ $ =xlnx|{1}{N}-\int_{1}{N}xdxlnx $ $ =NlnN-\int{1}^{N}x\cdot \frac{1}{x}dx $ $ =NlnN-x|_{1}^{N} $ $ =NlnN-N+1 $ $ \rightarrow NlnN-N $ Tayor公式 $ f(x)=f(x_{0})+f'(x_{0})(x-x_{0})+\frac{f''(x_{0})}{2!}(x-x_{0})+...+\frac{f{(n)}(x_{0})}{n!}(x-x_{0}){n}+R_{n}(x) $ 令x=0，得到： $ f(x)=f(0)+f'(0)x+\frac{f''(0)}{2!}x{2}+...+\frac{f{n}(0)}{n!}x{n}+o(x{n}) $ Taylor公式的应用： 数值计算：初等函数值计算（在原点上展开） $$ sinx=x-\frac{x{3}}{3!}+\frac{x{5}}{5!}-\frac{x{7}}{7!}+\frac{x{9}}{9!}+...+(-1){m-1}\frac{x{2m-1}}{(2m-1)!}+R_{2m} $$ $$ e{x}=1+x+\frac{x{2}}{2!}+\frac{x{3}}{3!}+\frac{x{4}}{4!}+...+\frac{x^{n}}{n!}+R_{n} $$ 在实践中，往往需要做一定程度上的变换。 例子：计算$ e^{x} $ $ x=k\cdot ln2+r $ , $ \left | r \right |\leqslant 0.5\cdot ln2 $ $$ e{x}=e{k\cdot ln2+r}=e^{k\cdot ln2}\cdot e^{r} =2^{k}\cdot e^{r} $$ 方向导数 如果函数$ z=f(x,y) $在点$ P(x,y) $是可微分的，那么，函数在该点沿任意方向$ L $的方向导数都存在，且有： $ \frac{\partial f}{\partial l}=\frac{\partial f}{\partial x}cos\varphi +\frac{\partial f}{\partial y}sin\varphi $ 其中，$ \varphi $为x轴到方向L的转角。 梯度 设函数$ z=f(x,y) $在平面区域D内具有一阶连续偏导数，则对于每一个点$P(x,y)\in D$,向量 $$ (\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}) $$ 为函数$ z=f(x,y) $在点$P(x,y)$的梯度，记作$ gradf(x) $ 梯度的方向是函数在该点变化最快的方向 ###概率论 概率表达式：$ P(x)\in [0,1] $ 若x为离散，则$ P(x=x_{0}) $表示$ x_{0} $发生的概率 若x为连续变量，则$ P(x=x_{0}) $表示$x_{0}$发生的概率密度 累计分布函数：$ \Phi (x)=P(x\leq x_{0}) $ $ \Phi (x) $一定为单调递增函数。 $ min(\Phi (x))=0 $,$ max(\Phi (x))=1 $ 思考：将值域为[0,1]的单增函数$ y=F(x) $看成X事件的累积概率函数，若$y=F(x)$可导，则$f(x)=F'(x)$为概率密度函数 概率公式 条件概率：$ P(A|B)=\frac{P(AB)}{P(B)} $ 全概率公式：$ P(A)=\sum_{i}P(A|B_{i})P(B_{i}) $ 贝叶斯(Bayes)公式：$ P(B_{i}|A)=\frac{P(A|B_{i})P(B_{i})}{\sum_{j}P(A|B_{i})P(B_{j})} $ 贝叶斯公式： 分布]]></content>
      <tags>
        <tag>机器学习基础</tag>
      </tags>
  </entry>
</search>
