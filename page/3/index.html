<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Buevara" type="application/atom+xml" />






<meta property="og:type" content="website">
<meta property="og:title" content="Buevara">
<meta property="og:url" content="http://Buevara.github.io/page/3/index.html">
<meta property="og:site_name" content="Buevara">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Buevara">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://Buevara.github.io/page/3/"/>





  <title>Buevara</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Buevara</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">To hard!To work!</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Buevara.github.io/2018/04/15/深度学习--Inception和Xception网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Buevara">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Buevara">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/15/深度学习--Inception和Xception网络/" itemprop="url">深度学习--Inception和Xception网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-15T18:41:05+08:00">
                2018-04-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>深度学习--Inception和Xception网络</h2>
<p>整理了一些Inception和Xception网络的资料：</p>
<blockquote>
<p>1.<a href="https://blog.csdn.net/yuanchheneducn/article/details/53045551" target="_blank" rel="noopener">Inception v1到v4演变</a></p>
<p>2.<a href="https://blog.csdn.net/u010402786/article/details/52433324" target="_blank" rel="noopener">网络模型中Inception的作用与结构全解析</a></p>
<p>3.<a href="https://www.jianshu.com/p/0583f1ba068c" target="_blank" rel="noopener">卷积神经网络工作原理研究-Inception结构研究</a></p>
<p>4.<a href="https://blog.csdn.net/xbinworld/article/details/61674836" target="_blank" rel="noopener">卷积神经网络结构变化</a></p>
<p>5.<a href="https://zhuanlan.zhihu.com/p/33999416" target="_blank" rel="noopener">关于「Inception」和「Xception」的那些事</a></p>
<p>6.<a href="https://blog.csdn.net/lynnandwei/article/details/53736235" target="_blank" rel="noopener">inception V4 与resnet</a></p>
<p>7.<a href="https://blog.csdn.net/u014380165/article/details/75142710" target="_blank" rel="noopener">Xception算法详解</a></p>
<p>8.<a href="https://zhuanlan.zhihu.com/p/29367273" target="_blank" rel="noopener">CNN卷及操作详解</a></p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Buevara.github.io/2018/04/13/机器学习--模型的评价指标/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Buevara">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Buevara">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/13/机器学习--模型的评价指标/" itemprop="url">机器学习--模型的评价指标</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-13T19:45:15+08:00">
                2018-04-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>分为回归问题的评价指标和分类问题的评价指标。</h2>
<blockquote>
<p>回归问题的评价指标：</p>
<blockquote>
<p>1.SSE（误差平方和）</p>
<p>2.MAE(Mean Absolute Error) 平均绝对误差</p>
<p>3.MSE(Mean Square Error)<strong>平均平方差/均方误差是回归任务最常用的性能度量</strong>
4.MAPE</p>
<p>5.R-square（决定系数）</p>
<p>6.Adjusted R-Square (校正决定系数）</p>
</blockquote>
</blockquote>
<blockquote>
<p>分类问题的评价指标：</p>
</blockquote>
<blockquote>
<blockquote>
<p>1.accuarcy 正确率</p>
<p>2.precision,recall,F1-score,F-score,ROC-AUC曲线</p>
</blockquote>
</blockquote>
<hr>
<h3>回归问题</h3>
<h4>1.SSE（误差平方和）</h4>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.14/4.14-1.png" alt="4.14-1"></p>
<h4>2.MAE(Mean Absolute Error) 平均绝对误差</h4>
<p>$$  MAE=\frac{1}{n}\sum_{n}^{i=1}|Y_predict-Y_actual|  $$</p>
<h4>3.MSE(Mean Square Error)</h4>
<p>$$ MSE=\frac{1}{n}\sum_{n}<sup>{i=1}(Y_predict-Y_actual)</sup>{2}  $$</p>
<h4>4.MAPE</h4>
<p>全称是Mean Absolute Percentage Error。
$$ MAPE=\frac{100}{n}\sum_{n}^{i=1}|\frac{Y_actual-Y_predict}{Y_actual}| $$</p>
<h4>5.R-square（决定系数）</h4>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.14/4.14-2.png" alt="4.14-2"></p>
<h4>6.Adjusted R-Square (校正决定系数）</h4>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.14/4.14-3.png" alt="4.14-3"></p>
<hr>
<h3>回归问题</h3>
<h4>1.accuarcy 正确率</h4>
<p>$$ accuarcy=\frac{N_true}{N_sum} $$
$N_true$是分类正确的数量
$N_sum$是训练样本的总数量</p>
<h4>2.precision,recall,F1-score,F-score,ROC-AUC曲线</h4>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.14/4.14-4.png" alt="4.14-4">
<img src="https://github.com/Buevara/blog_img/raw/master/shouhuatu/9-8.jpeg" alt="4.14-2018-09-08">
<img src="https://github.com/Buevara/blog_img/raw/master/4.14/4.14-5.png" alt="4.14-5"></p>
<p>ROC曲线如下图，横轴是负正率，纵轴是真正率。
<img src="https://github.com/Buevara/blog_img/raw/master/4.14/4.14-6.png" alt="4.14-6">
如何绘制曲线呢？</p>
<blockquote></blockquote>
<p>真实标签y=[1,1,0,0,1]
分类器预测=[0.5,0.6,0.55,0.4,0.7]
我们选择阈值0.1，那么5个样本被分进1类，选择0.3，结果是一样的，选择0.45，那么只有样本4分为0类，其余都是1类，得到类别，我们就能计算真正率和负正率，就可以画图了。</p>
<blockquote></blockquote>
<p>理解ROC曲线：</p>
<p>ROC曲线图中的四个点和一条线。第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。</p>
<p>对于ROC曲线图中的y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。</p>
<hr>
<p>参考相关博客和资料：</p>
<blockquote></blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Buevara.github.io/2018/04/12/面经--58同城算法岗位、优信二手车数据挖掘岗位/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Buevara">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Buevara">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/12/面经--58同城算法岗位、优信二手车数据挖掘岗位/" itemprop="url">面经--58同城算法岗位、优信二手车数据挖掘岗位</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-12T17:06:30+08:00">
                2018-04-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>面经--58同城算法岗位、优信二手车数据挖掘岗位</h2>
<p>58同城两面挂，优信二手车一面结束，所以放在一起整理。</p>
<hr>
<h3>58同城算法岗位</h3>
<h4>一面</h4>
<ul>
<li>
<p>1.自我介绍。</p>
</li>
<li>
<p>2.自己的海量数据处理项目。</p>
</li>
<li>
<p>3.kaggle项目中用到的神经网络的不同，和更优秀的模型的差别。</p>
</li>
<li>
<p>4.机器学习的基础模型中svm的原理，线性不可分时还可以用svm吗？可以，用高斯核函数，无限延伸到无穷维度。</p>
</li>
<li>
<p>5.bagging和boosting的区别？</p>
</li>
</ul>
<p><a href="https://buevara.github.io/2018/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0--%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95/">答案点击这里</a></p>
<p>一面结束，四十分钟左右，其中聊项目比较多，没问算法。</p>
<hr>
<h4>二面（电话面试）</h4>
<ul>
<li>
<p>1.自我介绍。</p>
</li>
<li>
<p>2.介绍自己最深刻的项目。（京东店铺销量预测大赛）</p>
</li>
<li>
<p>3.XGBOOST和GBDT的不同，为什么XGBOOST复杂度高却比GBDT更快？
<a href="https://blog.csdn.net/wolf963/article/details/78508858" target="_blank" rel="noopener">参考答案1</a>
<a href="https://www.zhihu.com/question/41354392" target="_blank" rel="noopener">参考答案2</a></p>
</li>
<li>
<p>4.python装饰器？</p>
</li>
</ul>
<p>装饰器：拓展原来函数功能的一种函数，这个函数的特殊之处在于它的返回值也是一个函数，使用python装饰器的好处就是在不用更改原函数的代码前提下给函数增加新的功能。
<a href="https://blog.csdn.net/u010358168/article/details/77773199" target="_blank" rel="noopener">参考答案</a></p>
<p>拓展：
迭代器：
它是一个带状态的对象，他能在你调用next()方法的时候返回容器中的下一个值，任何实现了__iter__和__next__()（python2中实现next()）方法的对象都是迭代器，__iter__返回迭代器自身，__next__返回容器中的下一个值，如果容器中没有更多元素了，则抛出StopIteration异常，至于它们到底是如何实现的这并不重要。
迭代器就是其他语言里的iterator，主要是实现了<code>__iter__</code>，以及和其他语言对应的<code>__next__</code>方法。</p>
<p>生成器：
是Python语言中最吸引人的特性之一，生成器其实是一种特殊的迭代器，不过这种迭代器更加优雅。它不需要再像上面的类一样写__iter__()和__next__()方法了，只需要一个yiled关键字。
生成器generator是python特有的，iterator的一个子类，（我理解）主要目的是方便你来实现一个iterator。</p>
<ul>
<li>5.为什么树模型不需要归一化特征？</li>
</ul>
<p><strong>概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。像svm、线性回归之类的最优化问题就需要归一化。决策树属于前者。</strong></p>
<ul>
<li>6.SVM用于回归模型如何解释？</li>
</ul>
<p><a href="http://blog.sina.com.cn/s/blog_62970c250102xfzj.html" target="_blank" rel="noopener">参考答案</a></p>
<ul>
<li>7.L1正则化和L2正则化特征，为什么？为什么L1和等值线的交点在坐标轴上的概率最大？
<a href="https://www.cnblogs.com/heguanyou/p/7688344.html" target="_blank" rel="noopener">参考答案</a>
<a href="https://www.zhihu.com/question/23536142" target="_blank" rel="noopener">参考答案</a>
<a href="https://blog.csdn.net/zhuxiaodong030/article/details/54408786" target="_blank" rel="noopener">参考答案</a>
<img src="https://github.com/Buevara/blog_img/raw/master/4.16/4.16-3.png" alt="4.16-3"></li>
<li>8.MySql数据库引擎是什么？</li>
</ul>
<p>二面结束，三十分钟左右，其中聊项目比较多，基础知识问的都只能说出一些，再往深我没想过，面试官说希望对模型有更深的理解。</p>
<hr>
<h3>优信二手车数据挖掘岗位</h3>
<h4>一面</h4>
<ul>
<li>
<p>1.自我介绍。</p>
</li>
<li>
<p>2.自己JD店铺预测项目如何做的，用了什么模型？SVM,XGBOOST,GBDT...</p>
</li>
<li>
<p>3.SVM有哪些超参数？惩罚系数C和核函数选择和核函数的$ \sigma $。</p>
</li>
</ul>
<blockquote>
<p>使用SVM时，有两个点要注意：
若使用核函数，一定要对Feature做Feature Scaling(Normalization)
若训练集m太小，但Feature数量n很大，则训练数据不足以拟合复杂的非线性模型，这种情况下只能用linear-kernel（就是fi=xi）不能用高斯核</p>
</blockquote>
<p>来自于 Andrew NG. machine learning class at coursera</p>
<ul>
<li>4.logistic回归的超参数？只有一个：学习率</li>
<li>5.针对于回归问题的评价指标？当时没答上来，应该是mse，mae，决定系数等等。</li>
<li>5.AUC和ROC怎么求？概念不是很清晰，没推导出来</li>
<li>6.bagging和boosting的区别？</li>
<li>7.bagging和boosting分别对模型的偏差和方差有着怎样的影响？
简单来说，就是bagging方差降低了，偏差高。boosting是对偏差进行拟合，降低了偏差，方差会随着训练增大。</li>
</ul>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.16/4.16-1.png" alt="4.16-1"></p>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.16/4.16-2.png" alt="4.16-2">
图片和结论来自于<a href="https://blog.csdn.net/shenxiaoming77/article/details/53894973" target="_blank" rel="noopener">Michael_Shentu的博客</a></p>
<ul>
<li>时间复杂度和空间复杂度的含义？</li>
<li>算法题：</li>
<li>1.求一个字符串的所有子集？ 简单递归思想</li>
<li>2.二分查找</li>
</ul>
<p>一面结束，五十分钟，答的一般，有两个问题不会。
一面结束让我等一下，进行二面，过了一会说二面不在，到时候再约面试，我说ok。
晚上hr小姐姐给我打电话，本来以为要约二面，结果和我说不用了，等下个月通知，一脸懵逼。</p>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Buevara.github.io/2018/04/09/机器学习--最大似然估计（MLE）和最大后验概率（MAP）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Buevara">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Buevara">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/09/机器学习--最大似然估计（MLE）和最大后验概率（MAP）/" itemprop="url">机器学习--最大似然估计（MLE）和最大后验概率（MAP）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-09T16:12:13+08:00">
                2018-04-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>最大似然估计（MLE）和最大后验概率（MAP）</h2>
<hr>
<h3>最大似然估计 MLE</h3>
<p>给定一堆数据，加入我们知道他是从某一分部中随机取出来的。可是我们并不知道这个分布具体的参，即“模型已定，参数未知”。例如，我们知道这个分布是正态分布，但是不知道均值和方差；或者是二项分布，但是不知道均值。最大似然估计（MLE，Maximum Likelihood Estimation）就可以用来估计模型的参数。MLE的目标是找出一组参数，使得模型产生出观测数据的概率最大：
$ \arg max _{ \mu}(p(X; \mu)) $</p>
<p>其中$ p(X; \mu) $ 就是似然函数，表示在参数$ \mu $ 下出现观测数据的概率。我们假设每个观测数据是独立的，那么有
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-1.png" alt="4.9-1"></p>
<p>为了寻求方便，一般对目标取$ log $。所以最优化似然函数等于最优化对数似然函数：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-2.png" alt="4.9-2">
举一个抛硬币的简单例子。 现在有一个正反面不是很匀称的硬币，如果正面朝上记为正，方面朝上记为反，抛10次的结果如下：</p>
<p>正正正反反反正反正反</p>
<p>很显然这个概率是0.5。现在我们用MLE的思想去求解它。我们知道每次抛硬币都是一次二项分布，设正面朝上的概率是$ \mu $，那么似然函数为：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-3.png" alt="4.9-3">
x=1表示正面朝上，x=0表示方面朝上。那么有：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-4.png" alt="4.9-4">
求导：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-5.png" alt="4.9-5">
令导数为0，很容易得到：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-6.png" alt="4.9-6">
也就是0.5。</p>
<hr>
<h3>最大后验概率  MAP</h3>
<p>以上求MLE   求得是找出一组能够使似然函数最大的参数，即$ \arg max <em>{ \mu}(p(X; \mu)) $ 。现在问题稍微复杂一点，假如这个参数$ \mu $ 有一个先验概率呢？比如说，在上面抛硬币的例子，假如我们的经验告诉我们，硬币一般都是均匀的，也就是$ \mu $=0.5的可能性最大，$ \mu $=0.2的可能性比较小，那么参数如何估计呢？这就是MAP要考虑的问题。MAP优化的是一个后验概率，即给定了观测值后是的$ \mu $最大的：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-7.png" alt="4.9-7">
把上式根据贝叶斯公式展开：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-8.png" alt="4.9-8">
我们可以看出第一项$ p(X; \mu) $ 就是似然函数，第二项$ p(\mu) $就是先验知识。取$ log $之后就是：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-9.png" alt="4.9-9">
回到刚才的抛硬币的例子，假设参数$ \mu $有一个先验估计，他服从$ Beta $分布，即：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-10.png" alt="4.9-10">
而每次抛硬币仍然服从二项分布：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-11.png" alt="4.9-11">
那么目标函数的导数是：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-12.png" alt="4.9-12">
求导的第一项已经在MLE中给出了，第二项为：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-13.png" alt="4.9-13">
令导数为0，求解为：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-14.png" alt="4.9-14">
其中，$ n</em>{H} $表示正面朝上的次数。这里看以看出，MLE与MAP的不同之处在于，MAP的结果多了一些先验分布的参数。</p>
<hr>
<h3>补充知识： Beta分布</h3>
<p>Beat分布是一种常见的先验分布，它形状由两个参数控制，定义域为[0,1]
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-15.png" alt="4.9-15"></p>
<p>Beta分布的最大值是x等于$ \frac{\alpha -1}{\alpha +\beta -2} $的时候：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-16.png" alt="4.9-16">
所以在抛硬币中，如果先验知识是说硬币是匀称的，那么就让$ \alpha = \beta  $。但是很显然即使它们相等，它两的值也对最终结果很有影响。它两的值越大，表示偏离匀称的可能性越小：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-17.png" alt="4.9-17"></p>
<hr>
<p>文章转自 <a href="http://www.cnblogs.com/sylvanas2012/p/5058065.html" target="_blank" rel="noopener">Leavingseason的博客</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Buevara.github.io/2018/04/05/机器学习--模型融合方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Buevara">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Buevara">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/05/机器学习--模型融合方法/" itemprop="url">机器学习--模型融合方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-05T11:32:11+08:00">
                2018-04-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3>Ensemble Learning--模型融合</h3>
<p>含义：
通过对多个单模型融合以提升整体性能。</p>
<blockquote>
<p>1.Voting</p>
<p>2.Averaging</p>
<p>3.Ranking</p>
<p>4.Bagging</p>
<p>5.Boosting</p>
<p>6.Bagging和Boosting的区别</p>
<p>7.Stacking</p>
<p>8.Blending</p>
<p>9.Stacking和Blending的区别</p>
</blockquote>
<h4>模型的融合条件</h4>
<ul>
<li>[x]  Base Model 之间的相关性要尽可能的小。这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。</li>
<li>[x]  Base Model 之间的性能表现不能差距太大。这其实是一个 Trade-off，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。</li>
</ul>
<hr>
<h4>1.Voting</h4>
<blockquote>
<p>投票即为，投票多者为最终的结果。
用于分类问题：
多个模型投票（当然可以设置权重）。最终投票数最多的类为最终被预测的类。</p>
</blockquote>
<hr>
<h4>2.Averaging</h4>
<blockquote>
<p>Averaging即所有预测器的结果平均。</p>
</blockquote>
<ul>
<li>[ ] 回归问题，直接取平均值作为最终的预测值。（也可以使用加权平均）</li>
<li>[ ] 分类问题，直接将模型的预测概率做平均。（or 加权）</li>
</ul>
<p>加权平均，其公式如下：
<img src="https://github.com/Buevara/blog_img/raw/master/4.11/4.11-2.png" alt="4.11-2"></p>
<p>其中n表示模型的个数， $Weighti$表示该模型权重，$Pi$表示模型i的预测概率值。</p>
<hr>
<h4>3.Ranking</h4>
<blockquote>
<p>Rank的思想其实和Averaging一致，但Rank是把排名做平均，对于例如一些评价指标有效，如：AUC指标。</p>
</blockquote>
<p>具体公式如下：</p>
<p>加权平均，其公式如下：</p>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.11/4.11-3.png" alt="4.11-3"></p>
<p>其中n表示模型的个数， $Weighti$表示该模型权重，所有权重相同表示平均融合。$Ranki$表示样本在第i个模型中的升序排名。它可以较快的利用排名融合多个模型之间的差异，而不需要加权融合概率。</p>
<hr>
<h4>4.Bagging</h4>
<blockquote>
<p>1.同一个学习算法在来自同一分布的多个不同的训练数据集上训练得到的模型偏差可能较大，即模型的方差（variance）较大，为了解决这个问题，可以综合多个模型的输出结果，对于回归问题可以取平均值，对于分类问题可以采取多数投票的方法。这就是Bagging的核心思想。
2.Bagging(Bootstrap Aggregation)是常用的统计学习方法，其综合的基本学习器可以是各种弱学习器。
3.使用训练数据的不同随机子集来训练每个 Base Model，最后每个 Base Model 权重相同，分类问题进行投票，回归问题平均。经典算法：随机森林</p>
</blockquote>
<p>注意： <strong>有放回抽样</strong> （可能抽到重复的样本）</p>
<p>Bagging是将弱分类器组装成强分类器的方法。
具体步骤：
A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）</p>
<p>B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）</p>
<p>C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）</p>
<p>如图所示：</p>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.11/4.11-4.png" alt="4.11-4"></p>
<p>要想综合N个弱分类器（决策树）的结果，我们需要采样N个训练数据集，在实际应用中获取N个训练数据集往往不现实，BootStrap 采样提供了一种有效的解决方法。
采用这样的方式解决了获取N个服从同一分布的原始数据集不现实的问题，而且在可接受程度上，可以认为Bootstrap 采样方式不影响到模型的准确性（以方差来衡量），即可以等价于使用N个不同的原始数据集。</p>
<blockquote>
<p>Bagging较单棵决策树来说，降低了<em>方差</em>，但由于将多棵决策树的结果进行了平均，这损失了模型的可解释性。</p>
</blockquote>
<hr>
<h4>5.boosting</h4>
<p>Boosting是一种迭代的方法，每一次训练会更关心上一次被分错的样本。</p>
<blockquote>
<p>核心问题：
1）在每一轮如何改变训练数据的权值或概率分布？</p>
</blockquote>
<blockquote>
<p>通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。</p>
</blockquote>
<blockquote>
<p>2）通过什么方式来组合弱分类器？</p>
</blockquote>
<blockquote>
<p>通过加法模型将弱分类器进行线性组合</p>
</blockquote>
<p>经典算法：</p>
<p>AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。</p>
<p>提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。</p>
<hr>
<h4>6.Bagging和Boosting的区别</h4>
<p>Bagging和Boosting的区别：</p>
<p>1）样本选择上：</p>
<p>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。</p>
<p>Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。</p>
<p>2）样例权重：</p>
<p>Bagging：使用均匀取样，每个样例的权重相等</p>
<p>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。</p>
<p>3）预测函数：</p>
<p>Bagging：所有预测函数的权重相等。</p>
<p>Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。</p>
<p>4）并行计算：</p>
<p>Bagging：各个预测函数可以并行生成</p>
<p>Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。</p>
<blockquote>
<p>这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。</p>
</blockquote>
<blockquote>
<p>下面是将决策树与这些算法框架进行结合所得到的新的算法：</p>
</blockquote>
<blockquote>
<p>1）Bagging + 决策树 = 随机森林</p>
</blockquote>
<blockquote>
<p>2）AdaBoost + 决策树 = 提升树</p>
</blockquote>
<blockquote>
<p>3）Gradient Boosting + 决策树 = GBDT</p>
</blockquote>
<hr>
<h4>7.Stacking</h4>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.11/4.11-5.jpg" alt="4.11-5">
图片来自于<a href="https://zhuanlan.zhihu.com/p/26890738" target="_blank" rel="noopener">Leon的知乎专栏</a>
个人认为很想深度学习模型中的xception网络，这里不同的模型相当于xception中不同尺寸的卷积，进行不同特征的提取。</p>
<p>我们在这里只考虑两层的stacking，多层同理。</p>
<p>假设第一层的用到3个模型model1，model2，model3，第二层一个模型model4。
步骤：</p>
<ol>
<li>首先先将训练集分成用K fold</li>
<li>用model对K-1折进行训练，剩下1折进行预测，一共可以的到K个（训练数据总数n/k）个结果，我们将其按照顺序排好，第1个n/k对应K折中的1折。reshape成为一个n*1的预测结果（我们可以把他考虑成为特征向量）</li>
<li>针对于预测集，K折交叉验证的每一次，都要对预测集进行一次预测，一共可以得到K*（预测集总数m），我们将其取平均，得到一个1*m的预测label（也可以看做是预测集的特征向量）</li>
<li>对于第一层的每个模型model1、model2、model3，我们都进行步骤1、2、3的操作，我们最终得到了训练集的特征向量是3*n（3是第一层的模型1数），得到预测集的特征向量是3*m。</li>
<li>对于第二层，我们用第一层得到的3*n作为特征用model4进行训练，模型训练结束后对
3*m进行预测，得到最终的结果。</li>
</ol>
<p>最顶层的模型一般是LR或者线性模型。</p>
<hr>
<h4>8.Blending</h4>
<blockquote>
<p>Blending与Stacking大致相同，只是Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模型的特征，而是建立一个Holdout集，例如说10%的训练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合。说白了，就是把Stacking流程中的K-Fold CV 改成 HoldOut CV。</p>
</blockquote>
<p>举例：
1.将数据划分成train,test，然后将train划分成不相交的两部分train_1,train_2</p>
<p>2.使用不同的模型对train_1训练，对train_2和test预测，生成两个1维向量，有多少模型就生成多少维向量</p>
<p>3.第二层使用前面模型对train_2生成的向量和label作为新的训练集，使用LR或者其他模型训练一个新的模型来预测test生成的向量</p>
<hr>
<h4>9.Stacking和Blending的区别</h4>
<ol>
<li>stacking由于加入了K-fold，更加复杂；blending不用K-fold，所以更加简单</li>
<li>stacking因为使用K-fold，所以训练集的数据分布和原数据集的不一样了，会引入估计偏差；而blending不会。</li>
<li>blending用的数据会过少，多层之后，有可能会过拟合；而stacking不会出现。</li>
</ol>
<p>两种方法都可以用下图来表示：
<img src="https://github.com/Buevara/blog_img/raw/master/4.11/4.11-6.png" alt="4.11-6"></p>
<hr>
<p>参考相关博客和资料：</p>
<blockquote>
<p>1: https://blog.csdn.net/shine19930820/article/details/75209021#11-voting</p>
<p>2: https://zhuanlan.zhihu.com/p/26890738</p>
<p>3: https://www.cnblogs.com/liuwu265/p/4690486.html</p>
<p>4: https://blog.csdn.net/sinat_29819401/article/details/71191219</p>
<p>5: https://blog.csdn.net/bryan__/article/details/51229032</p>
<p>6: https://blog.csdn.net/foolsnowman/article/details/51726007</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Buevara.github.io/2018/04/01/面经--瓜子二手车数据分析岗位/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Buevara">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Buevara">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/01/面经--瓜子二手车数据分析岗位/" itemprop="url">面经--瓜子二手车数据分析岗位</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-01T18:30:41+08:00">
                2018-04-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>面经--瓜子二手车数据分析岗位</h2>
<hr>
<p>面试约的上午九点半，去了直接面试，估计去的太早了。</p>
<hr>
<h3>一面</h3>
<ul>
<li>
<p>1.自我介绍.</p>
</li>
<li>
<p>2.介绍一下机器学习模型都有哪些，判别模型和生成模型区别，具体有哪些。</p>
</li>
<li>
<p>3.推导logistic的损失函数。</p>
</li>
<li>
<p>4.什么是过拟合，过拟合如何解决。</p>
</li>
<li>
<p>5.什么是gbdt，gbdt如何确定第一棵树，没答出来，她换了个问法，说给你两个特征x，x1是连续的，x2是非连续的，和对应的label，具体的构建gbdt的方法。</p>
</li>
<li>
<p>6.描述kmeans算法，难度加深，给定一个每个点和其他点的距离而不是坐标，该如何聚类。</p>
</li>
<li>
<p>7.最后一道算法题：不用内置函数求sqrt，给定x和 $\Sigma$, 其中$ \Sigma $是误差项。应该用二分查找，但是最后没写出来。</p>
</li>
</ul>
<p>一面结束，三十分钟左右。</p>
<hr>
<h3>二面</h3>
<ul>
<li>
<p>1.自我介绍。</p>
</li>
<li>
<p>2.什么是极大似然估计，如何推导。</p>
</li>
<li>
<p>3.什么是最大后验概率，如何推导。</p>
</li>
<li>
<p>4.描述一下朴素贝叶斯。</p>
</li>
<li>
<p>5.算法题：旋转数组中找到x。leetcode33题</p>
</li>
<li>
<p>6.1求概率：得分问题，赢一局得一分，输一局，不扣分，赢和输概率为p和q，n局得到m分的概率是。</p>
</li>
<li>
<p>6.2前面的问题难度加大，输一局扣一分，求n局得到m分的概率是。</p>
</li>
<li>
<p>6.3难度再提升，扣为0分不降分，n局得到m分的概率是。当时没做出来（应该用动态规划）</p>
</li>
<li>
<p>7.1用一枚硬币（1/2的概率）求出一个1/3的概率。</p>
</li>
<li>
<p>7.2用一枚硬币（1/2的概率）求出一个1/3的概率。</p>
</li>
<li>
<p>7.3用不均匀一枚硬币（概率不为1/2）求出一个1/3的概率。</p>
</li>
</ul>
<p>二面面试官问我有什么要问的，我问了一下他所在的部门是做什么的，都有什么方向，他说在做分类推荐。二面结束，共四十分钟左右，面试官很和善，但是他出的问题都不是我自己独立解答的，80%都需要他的一些指引，认识到了自己的不足，继续努力。</p>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Buevara.github.io/2018/04/01/机器学习--L1正则化项和L2正则化项理解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Buevara">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Buevara">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/01/机器学习--L1正则化项和L2正则化项理解/" itemprop="url">机器学习--L1正则化项和L2正则化项理解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-01T12:17:56+08:00">
                2018-04-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>L1正则化项和L2正则化项的理解</h2>
<hr>
<p>概念：</p>
<blockquote>
<p>L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为$ ||W||_{1} $</p>
</blockquote>
<blockquote>
<p>L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为$ ||W||_{2} $</p>
</blockquote>
<p><img src="https://github.com/Buevara/blog_img/raw/master/8.30/l1%E5%92%8Cl2%E5%88%86%E5%B8%83.png" alt="8.30"></p>
<p>先抛出个结论：</p>
<blockquote>
<p>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择,L1范数.
L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的正则项$ ||W||_{2} $最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，大比起1范数，更常用L2范数。
通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合；一定程度上，L1也可以防止过拟合</p>
</blockquote>
<hr>
<h3>L1正则化</h3>
<p>为什么L1正则化可以产生稀疏模型（L1是怎么让系数等于零的），以及为什么L2正则化可以防止过拟合？
假设有如下带L1正则化的损失函数：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-18.png" alt="4.9-18"></p>
<p>其中$ J0 $是原始的损失函数，加号后面的一项是$ L1 $正则化项，$ α $是正则化系数。注意到$ L1 $正则化是权值的绝对值之和，$J$是带有绝对值符号的函数，因此$J$是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数$J0$后添加$L1$正则化项时，相当于对$J0$做了一个约束。令$L=α∑w|w|$，则$J=J0+L$，此时我们的任务变成在$L$约束下求出$J0$取最小值的解。考虑二维的情况，即只有两个权值$w1$和$w2$，此时$L=|w1|+|w2|$对于梯度下降法，求解$J0$的过程可以画出等值线，同时$L1$正则化的函数$L$也可以在$w1w2$的二维平面上画出来。如下图
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-19.png" alt="4.9-19"></p>
<p>图中等值线是$J0$的等值线，黑色方形是$L$函数的图形。在图中，当$J0$等值线与$L$图形首次相交的地方就是最优解。上图中$J0$与$L$在$L$的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是$(w1,w2)=(0,w)$。可以直观想象，因为$L$函数有很多『突出的角』（二维情况下四个，多维情况下更多），$J0$与这些角接触的机率会远大于与$L$其它部位接触的机率，而在这些角上，会有很多权值等于$0$，这就是为什么$L1$正则化可以产生稀疏模型，进而可以用于特征选择。</p>
<p>而正则化前面的系数$α$，可以控制L图形的大小。$α$越小，$L$的图形越大（上图中的黑色方框）；$α$越大，$L$的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优点的值$(w1,w2)=(0,w)$中的$w$可以取到很小的值。</p>
<hr>
<h3>L2正则化</h3>
<p>假设有如下带L2正则化的损失函数：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-20.png" alt="4.9-20">
同样可以画出他们在二维平面上的图形，如下：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-21.png" alt="4.9-21">
二维平面下L2正则化的函数图形是个圆，与方形相比，被磨去了棱角。因此J0与L相交时使得w1或w2等于零的机率小了许多，这就是为什么L2正则化不具有稀疏性的原因。</p>
<hr>
<h3>L2正则化和过拟合</h3>
<p>拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。</p>
<blockquote>
<p>那为什么L2正则化可以获得值很小的参数？</p>
</blockquote>
<p>以线性回归中的梯度下降法为例。假设要求的参数为$θ$，$hθ(x)$是我们的假设函数，那么线性回归的代价函数如下：
以线性回归中的梯度下降法为例。假设要求的参数为$θ，hθ(x)$是我们的假设函数，那么线性回归的代价函数如下：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-22.png" alt="4.9-22">
那么在梯度下降法中，最终用于迭代计算参数θ的迭代式为：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-23.png" alt="4.9-23">
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-24.png" alt="4.9-24">
其中$α$是learning rate. 上式是没有添加L2正则化项的迭代公式，如果在原始代价函数之后添加L2正则化，则迭代公式会变成下面的样子：
<img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-25.png" alt="4.9-25">
其中$λ$就是正则化参数。从上式可以看到，与未添加$L2$正则化的迭代公式相比，每一次迭代，$θj$都要先乘以一个小于1的因子，从而使得$θj$不断减小，因此总得来看，$θ$是不断减小的。
最开始也提到$L1$正则化一定程度上也可以防止过拟合。之前做了解释，当$L1$的正则化系数很小时，得到的最优解会很小，可以达到和$L2$正则化类似的效果。</p>
<hr>
<h3>正则化参数的选择</h3>
<blockquote>
<p>通常越大的$λ$可以让代价函数在参数为$0$时取到最小值。</p>
</blockquote>
<p><img src="https://github.com/Buevara/blog_img/raw/master/4.9/MLE4.9-26.png" alt="4.9-26"></p>
<p>分别取$λ=0.5$和$λ=2$，可以看到越大的$λ$越容易使$F(x)$在$x=0$时取到最小值。</p>
<blockquote>
<p>$λ$越大，$θj$衰减得越快。另一个理解可以参考上图，$λ$越大，$L2$圆的半径越小，最后求得代价函数最值时各参数也会变得很小。</p>
</blockquote>
<h2>除了：L1和L2 regularization外，正则化常用的方法还有：数据集扩增、dropout</h2>
<p>文章转自 <a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">阿拉丁吃米粉的博客</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Buevara.github.io/2018/03/31/秋招面经--小米机器学习岗位/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Buevara">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Buevara">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/31/秋招面经--小米机器学习岗位/" itemprop="url">秋招面经--小米机器学习岗位</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-31T19:40:13+08:00">
                2018-03-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>秋招面经--小米机器学习算法岗位</h2>
<hr>
<p>说来也巧，春招第一家是小米，秋招小米是第二家，第一家是搜狗(已挂)。
面试约的下午两点，准时开始面试。</p>
<hr>
<h3>一面</h3>
<ul>
<li>
<p>1.自我介绍.</p>
</li>
<li>
<p>2.项目中用到的模型融合为什么不能无限的加全连接层？</p>
</li>
<li>
<p>我答全连接层越多会过拟合，面试官好像觉得我说的不对，他说是防止梯度爆炸或者梯度消失，存在疑惑？参考文献：https://blog.csdn.net/guoyunfei20/article/details/78283043</p>
</li>
<li>
<p>3.如何进行迁移训练，举例如何用猫狗分类的模型迁移到人脸项目中？</p>
</li>
<li>
<p>完全懵逼，我说直接取卷积层进行使用，他说还有别的方法，这样太粗暴了。可以用freeze的方式，一次训练某几层，freeze某几层。</p>
</li>
<li>
<p>回来查资料才发现应该是让我解答迁移学习和fine-tune：
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">迁移学习：</span><br><span class="line">迁移学习，是一种思想吧，并不是一种特定的算法或者模型，目的是将已学习到的知识应用到其他领域，提高在目标领域上的性能，比如说一个通用的语音模型迁移到某个人的语音识别，一个ImageNet上的图片分类问题迁移到医疗疾病识别上。</span><br><span class="line"></span><br><span class="line">而实现迁移学习最主要的方法则是finetune, 需要说明一点的是，很多人会把迁移学习和finetune混淆，其实两者是有区别的，finetune只是实现迁移学习的一种方法而已， 而finetune本身既可以用于迁移学习，也可以用于自身原有的训练学习，比如在已经初步训练好的一个模型基础上，用同样的数据再来训练微调，这也是finetune.而且，在深度学习领域，因为模型都比较复杂，train from scratch 一般需要耗费很长的时间，少则1-2周，多则1-2个月也是有可能的，如果没有GPU,那就不知道哪个猴年马月才能训练好，所以多数都是用finetune来训练的。</span><br><span class="line">    那么finetune该怎么使用呢？？一般可有如下几种情形：</span><br><span class="line"></span><br><span class="line">1. 自己的数据集比较小，和原先的数据集比较类似。首先，两者是比较类似的，所以可做浅层次的finetune，即只改最后几层，一般是只训练最后的分类层（数据集太少如果训练很多层的话，容易过拟合），而前面层的权重啥的不做改动，直接用来生成CNN feature。同时，因为数据集比较小，最好训练线性分类器。</span><br><span class="line"></span><br><span class="line">2. 自己的数据集比较大，和原先的数据集比价类似。由于数据集比较，所以可以训练整个网络，即finetune through the full network, 这样既不会过拟合也能取得不错的效果。那么什么是finetune through the full network呢？用原有的模型（pre-trained model）的参数来初始化，然后再结合当前的数据集继续训练。</span><br><span class="line"></span><br><span class="line">3. 自己的数据集比较小，而且和原先的数据集很不相同。如前所述，小数据集最好训练线性分类器，而同时两者又不相同，那么就不能只训练最后几层了，可以选取从中间或者更早的某一层开始训练，也不建议特别往前，因为数据集比较小太往前开始训练容易过拟合。</span><br><span class="line"></span><br><span class="line">4. 数据集比较大，而且和原先的数据集很不相同。这种情形我们可以train from sratch，就是完全从随机参数开始训练，不用迁移学习，不用以前别的模型的参数。其实，而且实际中，往往也会用迁移学习，用先前的模型的参数来初始化，然后再训练，即finetune through the full network。</span><br><span class="line"></span><br><span class="line">参考答案：</span><br><span class="line">https://www.zhihu.com/question/49534423</span><br><span class="line">https://blog.csdn.net/u010402786/article/details/70141261</span><br></pre></td></tr></table></figure></p>
</li>
<li>
<p>3.过拟合的主要原因？如何解决？为什么用L1和L2，L1和L2的原理，为什么有时同时用呢？</p>
</li>
<li>
<p>同时用的原因就是既可以筛除特性，又可以降低特征权值w，当时没打上来。唉！参考文献：https://www.cnblogs.com/yxwkf/p/5268577.html</p>
</li>
<li>
<p>4.xgboost的原理？</p>
</li>
<li>
<p>中间问我GBDT为什么要用梯度进行下降方向作为更新。他看我写公式不怎么熟练，就解释了霍夫曼编码和二叉树的联系，为什么xgboost要用泰勒展开式。我听得目瞪口呆，他说没事，互相讨论是进步的一种方式。</p>
</li>
<li>
<p>5.不同的损失函数有什么差别呢？为什么分类用交叉熵，用mse可以吗，那为什么不用mse或者hingeloss？？？</p>
</li>
<li>
<p>这个问题真的不会，他也没给我解释。</p>
</li>
<li>
<p>6.结束之前让我问他问题，我就好奇的问了他是不是搞理论的，他说是博士研究编码，之前在外企工作。</p>
</li>
</ul>
<p>一面结束，四十分钟左右。</p>
<hr>
<h3>二面</h3>
<p>面试官应该是工程专家（从发型就能看出来）：</p>
<ul>
<li>1.讲一下京东实习的内容，blabla讲了一堆，对我说不关心架构，只关心如何运行分布式，然后气氛突然很尴尬。</li>
<li>2.问了我的京东算法大赛，讲了一下思路，问了一下第一名的解题思路，差别有多大。</li>
<li>3.xgboost和GBDT的差别？</li>
<li>4.如何把算法封装，外部调用？一脸懵逼中。。。</li>
<li>5.算法题，普通数组中的topK大的数？</li>
<li>就是快速排序，问了我时间复杂度，我说nlogn，他说这道题的算法是吗？我说nlogk，让我继续写，写了不到十分钟，结束。</li>
<li>6.算法题，一个字符串的全排列？</li>
<li>基础递归，很简单</li>
<li>7.最后问我你的技术栈还有什么，让我想想还有什么优势，想了半天，啥也没说，最后临走问我期望工作地点，选北京还是武汉？我说都可以。</li>
</ul>
<p>二面结束，一个小时左右。总体面试还是很慌，毕竟才第二次。</p>
<hr>
<p>PS：搜狗面试全程项目，没什么营养，只有一道算法题有价值：</p>
<ul>
<li>10w条数据的语料库，每条2-3个词，给一句话‘我今天来搜狗面试’,从后向前分词，如‘我今天来搜狗面/试’，遇到语料库的词，提取，没有继续向前，若只剩一个字，直接提取，请写出程序？</li>
<li>用了最基础匹配算法，觉得肯定和10w有关，结果讲完就没有然后了。 之后就gg了。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Buevara</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">33</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Buevara</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
